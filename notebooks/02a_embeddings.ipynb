{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/02a_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAqJl9nG8Ifv"
      },
      "source": [
        "# Intro to embedding manipulation with `gensim`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use word embeddings, we will use a package called `gensim`. As it is not installed in Colab by default, we first need to install it using the package manager pip. The ! signifies that this is a system command, not python code."
      ],
      "metadata": {
        "id": "dm5VLnXc9Ydj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeiVHlEdyaUK"
      },
      "outputs": [],
      "source": [
        "!pip install gensim # restart after installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrj4r5L9F-F"
      },
      "source": [
        "## Word Embeddings from Pre-Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x2STAO9yFKr"
      },
      "outputs": [],
      "source": [
        "## we load a 100-dimensional GloVe model trained on Wikipedia data\n",
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-100') # small model so we don't have to wait too long..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQD-D8501bxe"
      },
      "outputs": [],
      "source": [
        "wv['ziti'] # what do our embeddings look like?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLSwVPvi9Mke"
      },
      "source": [
        "### Get most similar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEGA8j2vzGfd"
      },
      "outputs": [],
      "source": [
        "## so what are 'ziti' according to our model? Let's check the most similar embeddings:\n",
        "print(wv.most_similar(positive=['ziti'], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ht8QNXj7hkx"
      },
      "outputs": [],
      "source": [
        "## you can assess the similarity to other words with the `similarity` function (we'll cover what this score is later today)\n",
        "wv.similarity('ziti', 'penne')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21n1HoYg7-gr"
      },
      "outputs": [],
      "source": [
        "# Whereas\n",
        "wv.similarity('ziti', 'banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt5MwYbF75xj"
      },
      "outputs": [],
      "source": [
        "# and\n",
        "wv.similarity('ziti', 'car')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA2WUHbUz9E9"
      },
      "outputs": [],
      "source": [
        "## You can calculate with these embeddings:\n",
        "wv_london = wv['paris'] - wv['france'] + wv['england']\n",
        "print(wv.most_similar(positive=[wv_london], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAibSQeEzcLl"
      },
      "outputs": [],
      "source": [
        "## Though it does not always work perfectly\n",
        "wv_queen = wv['king'] + wv['woman'] - wv['man']\n",
        "print(wv.most_similar(positive=[wv_queen], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kFKdiTj3egc"
      },
      "outputs": [],
      "source": [
        "## you can also use the inbuilt function to get weighted averages\n",
        "print(wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIjhV2ZT_zM1"
      },
      "outputs": [],
      "source": [
        "## other cute functions\n",
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9xucGXr8TgJ"
      },
      "source": [
        "### Training your own Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfzwAqaBjKJ"
      },
      "source": [
        "We'll use an adapted dataset of dialogue in the Simpsons from [Kaggle](https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset?resource=download&select=simpsons_script_lines.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP4MONsoRGIi"
      },
      "source": [
        "![](https://media.giphy.com/media/v1.Y2lkPWVjZjA1ZTQ3cHA1dDZ5MWUwOWIyMmd3dHk3MGNyNGdvamEzc2w2dzVjdzdvMW5wOCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/tkYpAbKdWj4TS/giphy.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Ti4neS80A3"
      },
      "outputs": [],
      "source": [
        "## load data\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('https://www.dropbox.com/scl/fi/n5ffxvm4qyjkp8ws7qgoq/simpsons_script_lines_clean.csv?rlkey=gfliitwgi8cqsjxlcdmmdwtym&dl=1')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pf3Fg-zDakP"
      },
      "outputs": [],
      "source": [
        "## clean the texts (very rough here)\n",
        "dataset['cleaned_text'] = dataset['spoken_words'].str.replace('[^a-zA-Z ]','', regex=True) # remove anything that is not a letter or whitespace\n",
        "dataset['cleaned_text'] = dataset['cleaned_text'].str.lower() # lowercase\n",
        "dataset['cleaned_text'] = dataset['cleaned_text'].str.replace(' +', ' ', regex=True) # remove multiple whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xtfOvpZDss5"
      },
      "outputs": [],
      "source": [
        "dataset.cleaned_text.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djQKA9dhElNZ"
      },
      "outputs": [],
      "source": [
        "## filter empty rows and enforce string\n",
        "dataset = dataset[dataset['cleaned_text'] != '']\n",
        "dataset.loc[:,'cleaned_text'] = dataset['cleaned_text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FJq37IwF8AY"
      },
      "outputs": [],
      "source": [
        "## how many texts are left?\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg1eR-e0DLaL"
      },
      "outputs": [],
      "source": [
        "## create a list of lists of words by splitting along whitespaces\n",
        "sentences = [s.split(\" \") for s in dataset['cleaned_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSb0KZ4iE-K0"
      },
      "outputs": [],
      "source": [
        "sentences[1] # the pre-processing is not perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzZ0NPTdAaYm"
      },
      "outputs": [],
      "source": [
        "## load dataset (use an iterator for larger datasets: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model)\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "## estimate a model and save\n",
        "model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,  # number of dimensions of word embeddings\n",
        "    window=5,         # number of context words in either direction to include\n",
        "    min_count=5,      # how often must a word appear to enter the corpus?\n",
        "    workers=4         # how many CPUs should be used to fit the model?\n",
        "    )\n",
        "model.save(\"simpsons.w2v.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6i_YPrATIC7"
      },
      "outputs": [],
      "source": [
        "## what is the main output?\n",
        "model.wv.vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGoMMlvOFOCd"
      },
      "outputs": [],
      "source": [
        "## assess model\n",
        "model.wv.most_similar('homer', topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPaEChylUqHw"
      },
      "outputs": [],
      "source": [
        "## subset words of interest\n",
        "interesting_words = [\n",
        "    'banana', 'pineapple', 'mango',\n",
        "    'car', 'bike', 'motorcycle',\n",
        "    'bart', 'lisa', 'homer']\n",
        "interesting_vecs = wv[interesting_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8HBhdJhRpNw"
      },
      "outputs": [],
      "source": [
        "## dimensionality reduction using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(interesting_vecs)\n",
        "wv_2d = pca.transform(interesting_vecs)\n",
        "wv_2d = pd.DataFrame(wv_2d, index = interesting_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kEY6OgdTl6W"
      },
      "outputs": [],
      "source": [
        "wv_2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2BM66H6SNaQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(wv_2d[0], wv_2d[1])\n",
        "\n",
        "for i in wv_2d.index:\n",
        "    plt.annotate(i, (wv_2d[0][i], wv_2d[1][i]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW4MjQgkd3RG"
      },
      "outputs": [],
      "source": [
        "## subset words of interest\n",
        "interesting_words = [\n",
        "    'paris', 'berlin', 'france', 'germany']\n",
        "interesting_vecs = wv[interesting_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spbNdLA3d5cp"
      },
      "outputs": [],
      "source": [
        "## dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(interesting_vecs)\n",
        "wv_2d = pca.transform(interesting_vecs)\n",
        "wv_2d = pd.DataFrame(wv_2d, index = interesting_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKKVnGR5d7Ej"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "plt.scatter(wv_2d[0], wv_2d[1])\n",
        "\n",
        "for i in wv_2d.index:\n",
        "    plt.annotate(i, (wv_2d[0][i], wv_2d[1][i]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-5L4pnHCo4"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1) Calculate the word vector for Berlin from the vectors for 'paris', 'france', and 'germany'. Explain your reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcSEEROqHCo5"
      },
      "outputs": [],
      "source": [
        "## your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5M5op9HCo5"
      },
      "source": [
        "2) Assess the most similar words to this vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQweMGuPHCo5"
      },
      "outputs": [],
      "source": [
        "## your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isFhbcD9HCo5"
      },
      "source": [
        "3) Plot the calculated vector into the same vector space alongside paris, france, germany, and berlin using PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUWPTC7gHCo6"
      },
      "outputs": [],
      "source": [
        "## your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FDs1tXKHCo6"
      },
      "source": [
        "4) We have trained these embeddings on a dataset from the simpsons, a popular cartoon series. What consequences might this choice of data have for the word embeddings? How might this compare to a corpus trained on Wikipedia data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfNUE0CxHCo6"
      },
      "source": [
        "**Your answer**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FInZXwrHCo7"
      },
      "source": [
        "5) Try to find an interesting bias in the data by looking at word similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfWN38xoHCo7"
      },
      "outputs": [],
      "source": [
        "## your answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}