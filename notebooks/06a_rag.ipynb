{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1c3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentence similarity\n",
    "# Load sentence transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
    "    embeddings = sentence_model.encode([text1, text2])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Example texts for similarity comparison\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on the rug.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"I love pizza and pasta.\",\n",
    "    \"Italian food is delicious.\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“Š Text Similarity Matrix:\")\n",
    "print(\"Comparing different text pairs:\")\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        similarity = calculate_similarity(texts[i], texts[j])\n",
    "        print(f\"'{texts[i][:30]}...' vs '{texts[j][:30]}...': {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## semantic search\n",
    "def semantic_search(query, documents, top_k=3):\n",
    "    \"\"\"Find most similar documents to a query\"\"\"\n",
    "    query_embedding = sentence_model.encode([query])\n",
    "    doc_embeddings = sentence_model.encode(documents)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Example semantic search\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Computer vision enables machines to interpret visual information.\",\n",
    "    \"Reinforcement learning trains agents through trial and error.\"\n",
    "]\n",
    "\n",
    "query = \"How do computers understand language?\"\n",
    "search_results = semantic_search(query, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dynamic few-shot\n",
    "class DynamicFewShotPrompter:\n",
    "    def __init__(self, examples, sentence_model):\n",
    "        self.examples = examples\n",
    "        self.sentence_model = sentence_model\n",
    "\n",
    "    def get_relevant_examples(self, query, k=3):\n",
    "        \"\"\"Retrieve k most similar examples to the query\"\"\"\n",
    "        query_embedding = self.sentence_model.encode([query])\n",
    "        example_texts = [ex['input'] for ex in self.examples]\n",
    "        example_embeddings = self.sentence_model.encode(example_texts)\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, example_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "        return [self.examples[idx] for idx in top_indices]\n",
    "\n",
    "    def create_prompt(self, query, task_description, k=3):\n",
    "        \"\"\"Create a dynamic few-shot prompt\"\"\"\n",
    "        relevant_examples = self.get_relevant_examples(query, k)\n",
    "\n",
    "        prompt = f\"{task_description}\\n\\n\"\n",
    "\n",
    "        for ex in relevant_examples:\n",
    "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
    "\n",
    "        prompt += f\"Input: {query}\\nOutput:\"\n",
    "        return prompt\n",
    "\n",
    "# Example dataset for sentiment analysis\n",
    "sentiment_examples = [\n",
    "    {\"input\": \"I love this movie!\", \"output\": \"positive\"},\n",
    "    {\"input\": \"This food tastes terrible\", \"output\": \"negative\"},\n",
    "    {\"input\": \"The weather is nice today\", \"output\": \"positive\"},\n",
    "    {\"input\": \"I'm feeling sad\", \"output\": \"negative\"},\n",
    "    {\"input\": \"This book is okay\", \"output\": \"neutral\"},\n",
    "    {\"input\": \"Amazing service at this restaurant\", \"output\": \"positive\"},\n",
    "    {\"input\": \"The product broke after one day\", \"output\": \"negative\"},\n",
    "    {\"input\": \"Not bad, could be better\", \"output\": \"neutral\"},\n",
    "    {\"input\": \"Absolutely fantastic experience\", \"output\": \"positive\"},\n",
    "    {\"input\": \"Waste of money\", \"output\": \"negative\"}\n",
    "]\n",
    "\n",
    "# Initialize dynamic prompter\n",
    "prompter = DynamicFewShotPrompter(sentiment_examples, sentence_model)\n",
    "\n",
    "# Test dynamic prompting\n",
    "test_query = \"This pizza is incredibly delicious\"\n",
    "dynamic_prompt = prompter.create_prompt(\n",
    "    test_query,\n",
    "    \"Classify the sentiment of the following text as positive, negative, or neutral:\",\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
    "# corpus = [item for item in dataset[\"passages\"]]\n",
    "\n",
    "# Always clean + use this corpus consistently\n",
    "corpus = []\n",
    "for item in dataset[\"passages\"]:\n",
    "    text = str(item).strip()\n",
    "    if text:\n",
    "        corpus.append(text)\n",
    "\n",
    "# Embedding model\n",
    "print(\"Encoding corpus...\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, device='cpu')\n",
    "corpus_embeddings_np = corpus_embeddings.numpy()\n",
    "\n",
    "# FAISS index\n",
    "index = faiss.IndexFlatL2(corpus_embeddings_np.shape[1])\n",
    "index.add(corpus_embeddings_np)\n",
    "\n",
    "# Reranker model\n",
    "# reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Generator (choose one: local HF model or OpenAI)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=torch.float16)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150)\n",
    "\n",
    "@spaces.GPU\n",
    "def rag_pipeline(query):\n",
    "    # Embed query\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=True, device='cpu').numpy()\n",
    "\n",
    "    # Retrieve top-k from FAISS\n",
    "    D, I = index.search(query_embedding, k=5)\n",
    "    retrieved_docs = [corpus[idx] for idx in I[0]]\n",
    "    \n",
    "    print(\"Retrieved indices:\", I[0])\n",
    "    print(\"Retrieved docs:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(\"-\", repr(doc))\n",
    "\n",
    "    # # Rerank\n",
    "    # rerank_pairs = [[str(query), str(doc)] for doc in retrieved_docs]\n",
    "    # scores = reranker.predict(rerank_pairs)\n",
    "    # reranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), reverse=True)]\n",
    "\n",
    "    # Combine for context\n",
    "    context = \"\\n\\n\".join(retrieved_docs[:2])\n",
    "    prompt = f\"\"\"Answer the following question using the provided context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "\n",
    "    # Generate\n",
    "    response = generator(prompt)[0][\"generated_text\"]\n",
    "    return response.split(\"Answer:\")[-1].strip()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
