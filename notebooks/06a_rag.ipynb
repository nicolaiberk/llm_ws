{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/06a_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tpYlhhRUgXsU",
      "metadata": {
        "id": "tpYlhhRUgXsU"
      },
      "source": [
        "# Informed Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an earlier session, we have explored how to query generative models and how these queries can be enriched with examples (or 'context') to provide more information to the model in one- or few-shot queries. In these cases, we provided the *same* context disregarding the query entry. Today, we will see that model responses can be substantially improved by carefully selecting the context provided to the model."
      ],
      "metadata": {
        "id": "CpxCwvrY7kEF"
      },
      "id": "CpxCwvrY7kEF"
    },
    {
      "cell_type": "markdown",
      "id": "J0ohUe4ThmVZ",
      "metadata": {
        "id": "J0ohUe4ThmVZ"
      },
      "source": [
        "> ❗ ACTIVATE THE GPU BY SELECTING RUNTIME IN THE UPPER RIGHT > CONNECT TO RUNTIME > T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HNtMppQlga_f",
      "metadata": {
        "id": "HNtMppQlga_f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers datasets faiss-gpu-cu12 transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khx1F651gkyk",
      "metadata": {
        "id": "khx1F651gkyk"
      },
      "source": [
        "> ❗ RESTART THE NOTEBOOK (DROPDOWN NEXT TO RUN ALL > RESTART SESSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9q9nSkTKgzRa",
      "metadata": {
        "id": "9q9nSkTKgzRa"
      },
      "source": [
        "The [sentence-transformers](https://sbert.net/) library provides an ecosystem of models designed specifically for efficient embedding generation. It works very similar to transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SVn9mPt3f4ag",
      "metadata": {
        "id": "SVn9mPt3f4ag"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iGeFnesYi9LG",
      "metadata": {
        "id": "iGeFnesYi9LG"
      },
      "source": [
        "We load a pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dffabef",
      "metadata": {
        "id": "4dffabef"
      },
      "outputs": [],
      "source": [
        "similarity_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QD20gtKIjA9q",
      "metadata": {
        "id": "QD20gtKIjA9q"
      },
      "source": [
        "Then we encode some sentences of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ljGutlG8i0GK",
      "metadata": {
        "id": "ljGutlG8i0GK"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"The Great Wall of China was built over several dynasties, with most of the existing structure dating from the Ming Dynasty (1368-1644).\",\n",
        "    \"The blue whale's heart alone can weigh as much as an automobile and is roughly the size of a small car.\",\n",
        "    \"Studies show that the Dunning-Kruger effect causes people with low ability in a domain to overestimate their competence in that area.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gbIPakrrjRVU",
      "metadata": {
        "id": "gbIPakrrjRVU"
      },
      "source": [
        "And encode them as embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9HfxnqeEi2Yl",
      "metadata": {
        "id": "9HfxnqeEi2Yl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = similarity_model.encode(sentences)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EjX-aQLXjgHC",
      "metadata": {
        "id": "EjX-aQLXjgHC"
      },
      "source": [
        "We can then calculate the cosine similarity of the sentences with each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yzRaQBGCi3rF",
      "metadata": {
        "id": "yzRaQBGCi3rF"
      },
      "outputs": [],
      "source": [
        "# 3. Calculate the embedding similarities\n",
        "similarities = similarity_model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vX982RZzjxHd",
      "metadata": {
        "id": "vX982RZzjxHd"
      },
      "source": [
        "## Similarity Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bL2SFAA4jljR",
      "metadata": {
        "id": "bL2SFAA4jljR"
      },
      "source": [
        "This is particularly useful if we are searching something using a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678eed75",
      "metadata": {
        "id": "678eed75"
      },
      "outputs": [],
      "source": [
        "query = \"How large is a blue whales heart?\"\n",
        "query_embedding = similarity_model.encode([query])\n",
        "similarities = similarity_model.similarity(query_embedding, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5gCcfjQj_kC",
      "metadata": {
        "id": "b5gCcfjQj_kC"
      },
      "source": [
        "Looks good! Now we can then select the most similar context to add to the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jJ_QD1nplX0i",
      "metadata": {
        "id": "jJ_QD1nplX0i"
      },
      "outputs": [],
      "source": [
        "best_index = similarities.squeeze().argmax().item() # get the index of the highest similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GapL12k0l7RC",
      "metadata": {
        "id": "GapL12k0l7RC"
      },
      "source": [
        "We can now add this context to our query, providing the relevant information to our model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jqXe_FLaj-_4",
      "metadata": {
        "id": "jqXe_FLaj-_4"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer the Question.\"},\n",
        "    {\"role\": \"user\", \"content\": query},\n",
        "    {\"role\": \"system\", \"content\": \"Context: \" + sentences[best_index]}\n",
        "]\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's provide this prompt to the model and see how it responds (it will take a moment to load the model):"
      ],
      "metadata": {
        "id": "1MWa3ReKAC_S"
      },
      "id": "1MWa3ReKAC_S"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", dtype=torch.float16).to(device)"
      ],
      "metadata": {
        "id": "eIvLC3PI9ka_"
      },
      "id": "eIvLC3PI9ka_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tprompt,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ],
      "metadata": {
        "id": "UrYNIb4uABWy"
      },
      "id": "UrYNIb4uABWy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "J809FFF5-qLc"
      },
      "id": "J809FFF5-qLc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "66kGsu9b_zry"
      },
      "id": "66kGsu9b_zry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Augmented Generation"
      ],
      "metadata": {
        "id": "DrosvFVOEYTI"
      },
      "id": "DrosvFVOEYTI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This, of course is more useful when you have a larger set of information to choose from to provide the context. Let's therefore get a mini-version of wikipedia content to choose the relevant context from. This data is conveniently available on the huggingface hub:"
      ],
      "metadata": {
        "id": "m_L-kLGsAqdP"
      },
      "id": "m_L-kLGsAqdP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39dfac2",
      "metadata": {
        "id": "e39dfac2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see below, the data consists of different text passages from Wikipedia articles:"
      ],
      "metadata": {
        "id": "z7M1Q-N7BHVv"
      },
      "id": "z7M1Q-N7BHVv"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['passages'][1234]"
      ],
      "metadata": {
        "id": "NHQn6xMVBCI_"
      },
      "id": "NHQn6xMVBCI_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clean this corpus up a little bit and encode all texts to embeddings. We start by writing the cleaning function removing empty texts and writing all texts to a list:"
      ],
      "metadata": {
        "id": "wMwgkWWABm9e"
      },
      "id": "wMwgkWWABm9e"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "## cleanup function\n",
        "def clean_text(example):\n",
        "    text = example[\"passage\"]\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?;:'\\\"-]\", \"\", text)  # remove weird chars\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize spaces\n",
        "    example[\"passage\"] = text\n",
        "    return example"
      ],
      "metadata": {
        "id": "LSk-O1YOBuLe"
      },
      "id": "LSk-O1YOBuLe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And apply it to our texts:"
      ],
      "metadata": {
        "id": "f9icy1zoCP23"
      },
      "id": "f9icy1zoCP23"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(clean_text)"
      ],
      "metadata": {
        "id": "D53KCQraCB-u"
      },
      "id": "D53KCQraCB-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we remove empty texts and reset the index:"
      ],
      "metadata": {
        "id": "WpN-5rSOH6Y-"
      },
      "id": "WpN-5rSOH6Y-"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda example: example[\"passage\"].strip() != \"\")"
      ],
      "metadata": {
        "id": "8jYsnXmbH-CQ"
      },
      "id": "8jYsnXmbH-CQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the embedding model from above to generate the embeddings:"
      ],
      "metadata": {
        "id": "SfNLc9fDC_h6"
      },
      "id": "SfNLc9fDC_h6"
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = similarity_model.encode([text for text in dataset[\"passages\"]['passage']], convert_to_tensor=True).cpu().numpy()"
      ],
      "metadata": {
        "id": "cVoBSNuCCOCn"
      },
      "id": "cVoBSNuCCOCn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings.shape # we get our vectors"
      ],
      "metadata": {
        "id": "ybyKPStGEkZH"
      },
      "id": "ybyKPStGEkZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use a library called `faiss` to provide fast search through our vectors - this is especially important when we have large context datasets."
      ],
      "metadata": {
        "id": "M6rGwbl2HNWu"
      },
      "id": "M6rGwbl2HNWu"
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# FAISS index\n",
        "index = faiss.IndexFlatL2(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings)"
      ],
      "metadata": {
        "id": "CBOcpxy7F0s4"
      },
      "id": "CBOcpxy7F0s4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = similarity_model.encode([query], convert_to_tensor=True).to(device).cpu().numpy()"
      ],
      "metadata": {
        "id": "jKx123d5ItEq"
      },
      "id": "jKx123d5ItEq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top-k from FAISS\n",
        "D, I = index.search(query_embedding, k=5)\n",
        "retrieved_docs = [dataset['passages'][int(idx)]['passage'] for idx in I[0]]"
      ],
      "metadata": {
        "id": "l7f1BxQqHX7e"
      },
      "id": "l7f1BxQqHX7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n'.join(retrieved_docs)\n",
        "context"
      ],
      "metadata": {
        "id": "RbSRgA0VIfsH"
      },
      "id": "RbSRgA0VIfsH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer the Question. If no relevant information is provided in the context, respond with 'I cannot answer this question based on the provided context'.\"},\n",
        "    {\"role\": \"user\", \"content\": query},\n",
        "    {\"role\": \"context\", \"content\": context}\n",
        "]"
      ],
      "metadata": {
        "id": "GgGC4JB5JJvr"
      },
      "id": "GgGC4JB5JJvr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the chat template and provide it to the model:"
      ],
      "metadata": {
        "id": "NI7OnLbQHUX-"
      },
      "id": "NI7OnLbQHUX-"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tprompt,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ],
      "metadata": {
        "id": "sF7gQIyoJss3"
      },
      "id": "sF7gQIyoJss3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs, max_new_tokens=1000)"
      ],
      "metadata": {
        "id": "N2Sh4LK9J21y"
      },
      "id": "N2Sh4LK9J21y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "yifjfboCJ-rg"
      },
      "id": "yifjfboCJ-rg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}