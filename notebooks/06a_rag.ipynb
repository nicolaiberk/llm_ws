{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b371e96",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/06a_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tpYlhhRUgXsU",
      "metadata": {
        "id": "tpYlhhRUgXsU"
      },
      "source": [
        "# Informed Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a7a630",
      "metadata": {},
      "source": [
        "THIS NOTEBOOK IS IN DEVELOPMENT. CURRENTLY CANNOT INSTALL FAISS, FINALIZE NOTEBOOK AT A LATER POINT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J0ohUe4ThmVZ",
      "metadata": {
        "id": "J0ohUe4ThmVZ"
      },
      "source": [
        "> ❗ ACTIVATE THE GPU BY SELECTING RUNTIME IN THE UPPER RIGHT > CONNECT TO RUNTIME > T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HNtMppQlga_f",
      "metadata": {
        "id": "HNtMppQlga_f"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers datasets faiss-cpu  transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khx1F651gkyk",
      "metadata": {
        "id": "khx1F651gkyk"
      },
      "source": [
        "> ❗ RESTART THE NOTEBOOK (DROPDOWN NEXT TO RUN ALL > RESTART SESSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9q9nSkTKgzRa",
      "metadata": {
        "id": "9q9nSkTKgzRa"
      },
      "source": [
        "The [sentence-transformers](https://sbert.net/) library provides an ecosystem of models designed specifically for efficient embedding generation. It works very similar to transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SVn9mPt3f4ag",
      "metadata": {
        "id": "SVn9mPt3f4ag"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iGeFnesYi9LG",
      "metadata": {
        "id": "iGeFnesYi9LG"
      },
      "source": [
        "We load a pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dffabef",
      "metadata": {
        "id": "4dffabef"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QD20gtKIjA9q",
      "metadata": {
        "id": "QD20gtKIjA9q"
      },
      "source": [
        "Then we encode some sentences of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ljGutlG8i0GK",
      "metadata": {
        "id": "ljGutlG8i0GK"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"The Great Wall of China was built over several dynasties, with most of the existing structure dating from the Ming Dynasty (1368-1644).\",\n",
        "    \"The blue whale's heart alone can weigh as much as an automobile and is roughly the size of a small car.\",\n",
        "    \"Studies show that the Dunning-Kruger effect causes people with low ability in a domain to overestimate their competence in that area.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gbIPakrrjRVU",
      "metadata": {
        "id": "gbIPakrrjRVU"
      },
      "source": [
        "And encode them as embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9HfxnqeEi2Yl",
      "metadata": {
        "id": "9HfxnqeEi2Yl"
      },
      "outputs": [],
      "source": [
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EjX-aQLXjgHC",
      "metadata": {
        "id": "EjX-aQLXjgHC"
      },
      "source": [
        "We can then calculate the cosine similarity of the sentences with each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yzRaQBGCi3rF",
      "metadata": {
        "id": "yzRaQBGCi3rF"
      },
      "outputs": [],
      "source": [
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vX982RZzjxHd",
      "metadata": {
        "id": "vX982RZzjxHd"
      },
      "source": [
        "## Similarity Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bL2SFAA4jljR",
      "metadata": {
        "id": "bL2SFAA4jljR"
      },
      "source": [
        "This is particularly useful if we are searching something using a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678eed75",
      "metadata": {
        "id": "678eed75"
      },
      "outputs": [],
      "source": [
        "query = \"How large is a blue whales heart?\"\n",
        "query_embedding = model.encode([query])\n",
        "similarities = model.similarity(query_embedding, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5gCcfjQj_kC",
      "metadata": {
        "id": "b5gCcfjQj_kC"
      },
      "source": [
        "Looks good! Now we can then select the most similar context to add to the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jJ_QD1nplX0i",
      "metadata": {
        "id": "jJ_QD1nplX0i"
      },
      "outputs": [],
      "source": [
        "best_index = similarities.squeeze().argmax().item() # get the index of the highest similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jqXe_FLaj-_4",
      "metadata": {
        "id": "jqXe_FLaj-_4"
      },
      "outputs": [],
      "source": [
        "prompt = \"Answer the Question. \\nQuery:\" + query + \"\\nContext: \" + sentences[best_index]\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GapL12k0l7RC",
      "metadata": {
        "id": "GapL12k0l7RC"
      },
      "source": [
        "We can try and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39dfac2",
      "metadata": {
        "id": "e39dfac2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tksS1nmTl5Th",
      "metadata": {
        "id": "tksS1nmTl5Th"
      },
      "outputs": [],
      "source": [
        "# Always clean + use this corpus consistently\n",
        "corpus = []\n",
        "for item in dataset[\"passages\"]:\n",
        "    text = str(item).strip()\n",
        "    if text:\n",
        "        corpus.append(text)\n",
        "\n",
        "# Embedding model\n",
        "print(\"Encoding corpus...\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True, device='cpu')\n",
        "corpus_embeddings_np = corpus_embeddings.numpy()\n",
        "\n",
        "# FAISS index\n",
        "index = faiss.IndexFlatL2(corpus_embeddings_np.shape[1])\n",
        "index.add(corpus_embeddings_np)\n",
        "\n",
        "# Reranker model\n",
        "# reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# Generator (choose one: local HF model or OpenAI)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=torch.float16)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150)\n",
        "\n",
        "@spaces.GPU\n",
        "def rag_pipeline(query):\n",
        "    # Embed query\n",
        "    query_embedding = embedder.encode([query], convert_to_tensor=True, device='cpu').numpy()\n",
        "\n",
        "    # Retrieve top-k from FAISS\n",
        "    D, I = index.search(query_embedding, k=5)\n",
        "    retrieved_docs = [corpus[idx] for idx in I[0]]\n",
        "\n",
        "    print(\"Retrieved indices:\", I[0])\n",
        "    print(\"Retrieved docs:\")\n",
        "    for doc in retrieved_docs:\n",
        "        print(\"-\", repr(doc))\n",
        "\n",
        "    # # Rerank\n",
        "    # rerank_pairs = [[str(query), str(doc)] for doc in retrieved_docs]\n",
        "    # scores = reranker.predict(rerank_pairs)\n",
        "    # reranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), reverse=True)]\n",
        "\n",
        "    # Combine for context\n",
        "    context = \"\\n\\n\".join(retrieved_docs[:2])\n",
        "    prompt = f\"\"\"Answer the following question using the provided context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\"\"\n",
        "\n",
        "    # Generate\n",
        "    response = generator(prompt)[0][\"generated_text\"]\n",
        "    return response.split(\"Answer:\")[-1].strip()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
