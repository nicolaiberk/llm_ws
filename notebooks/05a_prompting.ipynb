{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06dae26a",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/05a_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd82ebca",
      "metadata": {
        "id": "dd82ebca"
      },
      "source": [
        "# Annotation with Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wFDXSRAAJhKp",
      "metadata": {
        "id": "wFDXSRAAJhKp"
      },
      "source": [
        "Today, we are going to see how to generate text and annotations with generative LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7FfjwewmJrpN",
      "metadata": {
        "id": "7FfjwewmJrpN"
      },
      "source": [
        "> ❗ ACTIVATE THE GPU BY SELECTING RUNTIME IN THE UPPER RIGHT > CONNECT TO RUNTIME > T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jjlIuJ_3KGz8",
      "metadata": {
        "collapsed": true,
        "id": "jjlIuJ_3KGz8"
      },
      "outputs": [],
      "source": [
        "  !pip install transformers accelerate setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qEEOTKYTKIg6",
      "metadata": {
        "id": "qEEOTKYTKIg6"
      },
      "source": [
        "> ❗ RESTART THE NOTEBOOK (DROPDOWN NEXT TO RUN ALL > RESTART SESSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c457798d",
      "metadata": {
        "id": "c457798d"
      },
      "source": [
        "## Generating text with generative models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hoh07fK5KfMp",
      "metadata": {
        "id": "hoh07fK5KfMp"
      },
      "source": [
        "We will start by simply generating some text using a family of small generative models developed by huggingface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204c07f3",
      "metadata": {
        "id": "204c07f3"
      },
      "source": [
        "### Simple Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SQQgSc39Ku3v",
      "metadata": {
        "id": "SQQgSc39Ku3v"
      },
      "source": [
        "The all-powerful `pipeline` is again the simplest way to get inference running quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f84446",
      "metadata": {
        "id": "d2f84446"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "messages = [\n",
        "    \"Let me tell you a story. Once upon a time,\"\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4a63f1",
      "metadata": {
        "id": "eb4a63f1"
      },
      "source": [
        "### Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MAUcJkbhLE2e",
      "metadata": {
        "id": "MAUcJkbhLE2e"
      },
      "source": [
        "Many applications of LLMs require a chat template. We can use the tokenizer to enforce this template. The template simply indicates which parts of the text are from to the user and which are/should be from the assistant.\n",
        "\n",
        "Remember: LLM chats are just roleplay with special tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bde39a",
      "metadata": {
        "id": "83bde39a"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f_NxCJ7oMdAR",
      "metadata": {
        "id": "f_NxCJ7oMdAR"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "tokenized_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bFE_XaCbM-P4",
      "metadata": {
        "id": "bFE_XaCbM-P4"
      },
      "source": [
        "In this context, it is helpful to add the generation prompt indicating that the text should be generated in the role of the assistant. Otherwise, the model might generate more text as the user instead ([more](https://huggingface.co/docs/transformers/en/chat_templating?template=Mistral#addgenerationprompt))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-JiKRJGvMy9t",
      "metadata": {
        "id": "-JiKRJGvMy9t"
      },
      "outputs": [],
      "source": [
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CN3k4tGlM2sR",
      "metadata": {
        "id": "CN3k4tGlM2sR"
      },
      "outputs": [],
      "source": [
        "tokenized_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMN09_i_MAGT",
      "metadata": {
        "id": "cMN09_i_MAGT"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhkjNVKNN5qm",
      "metadata": {
        "id": "zhkjNVKNN5qm"
      },
      "outputs": [],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0TkDgyfLXl6",
      "metadata": {
        "id": "b0TkDgyfLXl6"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=30) # note the max_new_tokens parameter\n",
        "print(tokenizer.decode(outputs[0])) # note that the entire conversation is returned, including the system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0c50f5",
      "metadata": {
        "id": "7a0c50f5"
      },
      "source": [
        "### Zero-shot prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L29OwQklLaN7",
      "metadata": {
        "id": "L29OwQklLaN7"
      },
      "source": [
        "In order to get proper annotations from our model, we can simply ask the model to generate the relevant outputs. This is as simple as writing a prompt. Remember the best practices we discussed earlier today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad546d0",
      "metadata": {
        "id": "dad546d0"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"}\n",
        "    ],\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"The weather is horrible today!\"\n",
        "    \"\"\"}]\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9LqPh91EPVQ6",
      "metadata": {
        "id": "9LqPh91EPVQ6"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))\n",
        "print(tokenizer.decode(outputs[1][inputs['input_ids'].shape[1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8459dd43",
      "metadata": {
        "id": "8459dd43"
      },
      "source": [
        "### Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6eb633",
      "metadata": {
        "id": "6a6eb633"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about artificial intelligence or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Example:\n",
        "    \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "    Example:\n",
        "    \"The weather is horrible today!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"NOT AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "     Text: \"The impact of the new wave on automation on the labour market is not yet clear.\"\n",
        "     \"\"\"}\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5gXK7SScR-MQ",
      "metadata": {
        "id": "5gXK7SScR-MQ"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WIT81DDTR_t7",
      "metadata": {
        "id": "WIT81DDTR_t7"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88fbef9",
      "metadata": {},
      "source": [
        "# Informed Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07dd62f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9065176d",
      "metadata": {},
      "source": [
        "We load a pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a497b297",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4217413",
      "metadata": {},
      "source": [
        "Then we encode some sentences of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95855181",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"The Great Wall of China was built over several dynasties, with most of the existing structure dating from the Ming Dynasty (1368-1644).\",\n",
        "    \"The blue whale's heart alone can weigh as much as an automobile and is roughly the size of a small car.\",\n",
        "    \"Studies show that the Dunning-Kruger effect causes people with low ability in a domain to overestimate their competence in that area.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38dfcdf3",
      "metadata": {},
      "source": [
        "And encode them as embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c7df81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f47e2dd",
      "metadata": {},
      "source": [
        "We can then calculate the cosine similarity of the sentences with each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d632a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7dbb7bb",
      "metadata": {},
      "source": [
        "## Similarity Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d3012b",
      "metadata": {},
      "source": [
        "This is particularly useful if we are searching something using a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44437085",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How large is a blue whales heart?\"\n",
        "query_embedding = model.encode([query])\n",
        "similarities = model.similarity(query_embedding, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daa2af0",
      "metadata": {},
      "source": [
        "Looks good! Now we can then select the most similar context to add to the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e543c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_index = similarities.squeeze().argmax().item() # get the index of the highest similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179d6ca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Answer the Question. \\nQuery:\" + query + \"\\nContext: \" + sentences[best_index]\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b752e701",
      "metadata": {
        "id": "b752e701"
      },
      "source": [
        "### BONUS: Setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471dda47",
      "metadata": {},
      "source": [
        "Setfit is a particularly efficient solution for few-shot learning. YOu can find a brief explainer with code [here](https://huggingface.co/blog/setfit)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
