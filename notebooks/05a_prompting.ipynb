{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/05a_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd82ebca",
      "metadata": {
        "id": "dd82ebca"
      },
      "source": [
        "# Annotation with Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wFDXSRAAJhKp",
      "metadata": {
        "id": "wFDXSRAAJhKp"
      },
      "source": [
        "Today, we are going to see how to generate text and annotations with generative LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7FfjwewmJrpN",
      "metadata": {
        "id": "7FfjwewmJrpN"
      },
      "source": [
        "> ❗ ACTIVATE THE GPU BY SELECTING RUNTIME IN THE UPPER RIGHT > CONNECT TO RUNTIME > T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jjlIuJ_3KGz8",
      "metadata": {
        "collapsed": true,
        "id": "jjlIuJ_3KGz8"
      },
      "outputs": [],
      "source": [
        "  !pip install transformers accelerate setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qEEOTKYTKIg6",
      "metadata": {
        "id": "qEEOTKYTKIg6"
      },
      "source": [
        "> ❗ RESTART THE NOTEBOOK (DROPDOWN NEXT TO RUN ALL > RESTART SESSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c457798d",
      "metadata": {
        "id": "c457798d"
      },
      "source": [
        "## Generating text with generative models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hoh07fK5KfMp",
      "metadata": {
        "id": "hoh07fK5KfMp"
      },
      "source": [
        "We will start by simply generating some text using a family of small generative models developed by huggingface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204c07f3",
      "metadata": {
        "id": "204c07f3"
      },
      "source": [
        "### Simple Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SQQgSc39Ku3v",
      "metadata": {
        "id": "SQQgSc39Ku3v"
      },
      "source": [
        "The all-powerful `pipeline` is again the simplest way to get inference running quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f84446",
      "metadata": {
        "id": "d2f84446"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "messages = [\n",
        "    \"Let me tell you a story. Once upon a time,\"\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4a63f1",
      "metadata": {
        "id": "eb4a63f1"
      },
      "source": [
        "### Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MAUcJkbhLE2e",
      "metadata": {
        "id": "MAUcJkbhLE2e"
      },
      "source": [
        "Many applications of LLMs require a chat template. We can use the tokenizer to enforce this template. The template simply indicates which parts of the text are from to the user and which are/should be from the assistant.\n",
        "\n",
        "Remember: LLM chats are just roleplay with special tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bde39a",
      "metadata": {
        "id": "83bde39a"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f_NxCJ7oMdAR",
      "metadata": {
        "id": "f_NxCJ7oMdAR"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "tokenized_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bFE_XaCbM-P4",
      "metadata": {
        "id": "bFE_XaCbM-P4"
      },
      "source": [
        "In this context, it is helpful to add the generation prompt indicating that the text should be generated in the role of the assistant. Otherwise, the model might generate more text as the user instead ([more](https://huggingface.co/docs/transformers/en/chat_templating?template=Mistral#addgenerationprompt))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-JiKRJGvMy9t",
      "metadata": {
        "id": "-JiKRJGvMy9t"
      },
      "outputs": [],
      "source": [
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CN3k4tGlM2sR",
      "metadata": {
        "id": "CN3k4tGlM2sR"
      },
      "outputs": [],
      "source": [
        "tokenized_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMN09_i_MAGT",
      "metadata": {
        "id": "cMN09_i_MAGT"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhkjNVKNN5qm",
      "metadata": {
        "id": "zhkjNVKNN5qm"
      },
      "outputs": [],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0TkDgyfLXl6",
      "metadata": {
        "id": "b0TkDgyfLXl6"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=100) # note the max_new_tokens parameter\n",
        "print(tokenizer.decode(outputs[0])) # note that the entire conversation is returned, including the system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0c50f5",
      "metadata": {
        "id": "7a0c50f5"
      },
      "source": [
        "### Zero-shot prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L29OwQklLaN7",
      "metadata": {
        "id": "L29OwQklLaN7"
      },
      "source": [
        "In order to get proper annotations from our model, we can simply ask the model to generate the relevant outputs. This is as simple as writing a prompt. Remember the best practices we discussed earlier today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad546d0",
      "metadata": {
        "id": "dad546d0"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"}\n",
        "    ],\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"The weather is horrible today!\"\n",
        "    \"\"\"}]\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "oothyf36NLaB"
      },
      "id": "oothyf36NLaB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "TpbEgwmeLMFy"
      },
      "id": "TpbEgwmeLMFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9LqPh91EPVQ6",
      "metadata": {
        "id": "9LqPh91EPVQ6"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))\n",
        "print(tokenizer.decode(outputs[1][inputs['input_ids'].shape[1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8459dd43",
      "metadata": {
        "id": "8459dd43"
      },
      "source": [
        "### Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6eb633",
      "metadata": {
        "id": "6a6eb633"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about artificial intelligence or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Example:\n",
        "    \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "    Example:\n",
        "    \"The weather is horrible today!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"NOT AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "     Text: \"The impact of the new wave on automation on the labour market is not yet clear.\"\n",
        "     \"\"\"}\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "tokenized_chat"
      ],
      "metadata": {
        "id": "bJfjkUBgMtsu"
      },
      "id": "bJfjkUBgMtsu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5gXK7SScR-MQ",
      "metadata": {
        "id": "5gXK7SScR-MQ"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WIT81DDTR_t7",
      "metadata": {
        "id": "WIT81DDTR_t7"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88fbef9",
      "metadata": {
        "id": "c88fbef9"
      },
      "source": [
        "# Informed Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07dd62f",
      "metadata": {
        "id": "a07dd62f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9065176d",
      "metadata": {
        "id": "9065176d"
      },
      "source": [
        "We load a pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a497b297",
      "metadata": {
        "id": "a497b297"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4217413",
      "metadata": {
        "id": "d4217413"
      },
      "source": [
        "Then we encode some sentences of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95855181",
      "metadata": {
        "id": "95855181"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"The Great Wall of China was built over several dynasties, with most of the existing structure dating from the Ming Dynasty (1368-1644).\",\n",
        "    \"The blue whale's heart alone can weigh as much as an automobile and is roughly the size of a small car.\",\n",
        "    \"Studies show that the Dunning-Kruger effect causes people with low ability in a domain to overestimate their competence in that area.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38dfcdf3",
      "metadata": {
        "id": "38dfcdf3"
      },
      "source": [
        "And encode them as embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c7df81",
      "metadata": {
        "id": "f7c7df81"
      },
      "outputs": [],
      "source": [
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f47e2dd",
      "metadata": {
        "id": "5f47e2dd"
      },
      "source": [
        "We can then calculate the cosine similarity of the sentences with each other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d632a86",
      "metadata": {
        "id": "8d632a86"
      },
      "outputs": [],
      "source": [
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7dbb7bb",
      "metadata": {
        "id": "e7dbb7bb"
      },
      "source": [
        "## Similarity Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d3012b",
      "metadata": {
        "id": "02d3012b"
      },
      "source": [
        "This is particularly useful if we are searching something using a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44437085",
      "metadata": {
        "id": "44437085"
      },
      "outputs": [],
      "source": [
        "query = \"How large is a blue whales heart?\"\n",
        "query_embedding = model.encode([query])\n",
        "similarities = model.similarity(query_embedding, embeddings)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daa2af0",
      "metadata": {
        "id": "7daa2af0"
      },
      "source": [
        "Looks good! Now we can then select the most similar context to add to the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e543c1",
      "metadata": {
        "id": "72e543c1"
      },
      "outputs": [],
      "source": [
        "best_index = similarities.squeeze().argmax().item() # get the index of the highest similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179d6ca4",
      "metadata": {
        "id": "179d6ca4"
      },
      "outputs": [],
      "source": [
        "prompt = \"Answer the Question. \\nQuery:\" + query + \"\\nContext: \" + sentences[best_index]\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b752e701",
      "metadata": {
        "id": "b752e701"
      },
      "source": [
        "### BONUS: Setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471dda47",
      "metadata": {
        "id": "471dda47"
      },
      "source": [
        "Setfit is a particularly efficient solution for few-shot learning. YOu can find a brief explainer with code [here](https://huggingface.co/blog/setfit)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "lpv5d6s6Q1MG"
      },
      "id": "lpv5d6s6Q1MG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Think of a concept of interest for your research. Operationalize it with some labels. Define some example texts with the associated labels for each category. These texts will serve as the context explaining to our model how to annotate."
      ],
      "metadata": {
        "id": "G_UrgXu9RBt7"
      },
      "id": "G_UrgXu9RBt7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the context\n",
        "annotated_news_articles = [\n",
        "    \"Border patrol agents apprehended over 2,000 migrants attempting to cross the southern border illegally last week. The surge comes amid renewed debates over immigration policy reform in Congress.\",\n",
        "    \"A new study reveals that climate change is accelerating the melting of Antarctic ice sheets at an unprecedented rate. Scientists warn this could lead to significant sea level rise within the next decade.\",\n",
        "    \"The Federal Reserve announced a 0.25% interest rate cut to stimulate economic growth amid concerns about inflation. Market analysts expect this move to boost consumer spending during the holiday season.\",\n",
        "    \"Immigration courts are facing a backlog of over 3 million cases, with average wait times exceeding four years. Legal advocates are calling for increased funding to hire more immigration judges.\",\n",
        "    \"Tech giant announces breakthrough in quantum computing that could revolutionize data processing capabilities. The new chip design promises to solve complex problems exponentially faster than traditional computers.\",\n",
        "    \"A bipartisan group of senators introduced legislation to streamline the legal immigration process for skilled workers. The bill aims to reduce visa processing times and increase annual caps for certain categories.\",\n",
        "    \"Wildfire season has started earlier than expected across the western United States, with three major blazes already burning thousands of acres. Drought conditions and high temperatures are contributing to the increased fire risk.\",\n",
        "    \"New archaeological discoveries in Egypt have uncovered a previously unknown pharaoh's tomb dating back 3,400 years. The tomb contains well-preserved artifacts that could reshape understanding of ancient Egyptian history.\",\n",
        "    \"Local authorities rescued 45 undocumented immigrants from a suspected human trafficking operation in a warehouse outside Houston. The investigation has led to multiple arrests and is ongoing.\",\n",
        "    \"Major automaker recalls 500,000 vehicles due to faulty brake systems that could increase accident risk. The company will provide free repairs at authorized dealerships nationwide.\"\n",
        "]\n",
        "\n",
        "# Associated labels\n",
        "labels = [\n",
        "    \"immigration\",\n",
        "    \"NOT immigration\",\n",
        "    \"NOT immigration\",\n",
        "    \"immigration\",\n",
        "    \"NOT immigration\",\n",
        "    \"immigration\",\n",
        "    \"NOT immigration\",\n",
        "    \"NOT immigration\",\n",
        "    \"immigration\",\n",
        "    \"NOT immigration\"\n",
        "]"
      ],
      "metadata": {
        "id": "Q_psTeu6Z6xD"
      },
      "id": "Q_psTeu6Z6xD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a query to pass to our model and an example text you wish to classify."
      ],
      "metadata": {
        "id": "T5ouRqQXRO0l"
      },
      "id": "T5ouRqQXRO0l"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "You are an expert annotator of news content, having years of experience as research assistant in social science projects.\n",
        "You assess whether the below text is about immigration.\n",
        "Only aswer with \"immigration\" or \"NOT immigration\". Do not provide an explanation.\n",
        "\n",
        "Here is the text:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c5vLIWsWaO0i"
      },
      "id": "c5vLIWsWaO0i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_to_classify = \"Asylum seekers at the U.S.-Mexico border are experiencing longer wait times due to new processing requirements implemented by immigration officials. Advocacy groups report that families are waiting up to six months in temporary shelters before their initial hearings.\""
      ],
      "metadata": {
        "id": "y1ZutJVXaMaE"
      },
      "id": "y1ZutJVXaMaE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using the sentence-transformer model, assess how similar each example is to the text you wish to classify."
      ],
      "metadata": {
        "id": "zLdTztQmR2-A"
      },
      "id": "zLdTztQmR2-A"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "wVnonlYbbG3n"
      },
      "id": "wVnonlYbbG3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_embeddings = model.encode(annotated_news_articles)"
      ],
      "metadata": {
        "id": "YifqSeWMbDek"
      },
      "id": "YifqSeWMbDek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = model.encode([article_to_classify]) ## note that we do NOT use the query here as we are interested in the example we annotate\n",
        "similarities = model.similarity(query_embedding, context_embeddings)"
      ],
      "metadata": {
        "id": "D8YLUELVaqom"
      },
      "id": "D8YLUELVaqom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities"
      ],
      "metadata": {
        "id": "u-scUkypbi3q"
      },
      "id": "u-scUkypbi3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Add the most similar example with the right annotation to the prompt. If you can, use the chat template from above."
      ],
      "metadata": {
        "id": "7S4vdg7dSBEt"
      },
      "id": "7S4vdg7dSBEt"
    },
    {
      "cell_type": "code",
      "source": [
        "best_index = similarities.squeeze().argmax().item() # get the position of the most similar embedding\n",
        "best_index"
      ],
      "metadata": {
        "id": "jjbXv_iQbn1Y"
      },
      "id": "jjbXv_iQbn1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = annotated_news_articles[best_index]\n",
        "example_label = labels[best_index]"
      ],
      "metadata": {
        "id": "c0Jmtva-btkD"
      },
      "id": "c0Jmtva-btkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text"
      ],
      "metadata": {
        "id": "_jEhRReZb6y4"
      },
      "id": "_jEhRReZb6y4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = [\n",
        "    {\"role\": \"user\", \"content\": query + example_text},\n",
        "     {\"role\": \"assistant\", \"content\": example_label},\n",
        "     {\"role\": \"user\", \"content\": article_to_classify}\n",
        "    ]"
      ],
      "metadata": {
        "id": "MWbd-CUdb9Vd"
      },
      "id": "MWbd-CUdb9Vd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template"
      ],
      "metadata": {
        "id": "1EC5zLhKcSm3"
      },
      "id": "1EC5zLhKcSm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Post the message to the model. Are you happy with the model's annotation?"
      ],
      "metadata": {
        "id": "sJvUDY6gSKyR"
      },
      "id": "sJvUDY6gSKyR"
    },
    {
      "cell_type": "code",
      "source": [
        "## model definition\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
      ],
      "metadata": {
        "id": "L28qG_UAcvIE"
      },
      "id": "L28qG_UAcvIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenization\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tchat_template,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ],
      "metadata": {
        "id": "GmPieoNsctq-"
      },
      "id": "GmPieoNsctq-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## inference\n",
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))"
      ],
      "metadata": {
        "id": "YL3XfgfOcwfD"
      },
      "id": "YL3XfgfOcwfD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}