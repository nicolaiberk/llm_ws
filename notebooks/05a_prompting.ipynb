{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/05a_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd82ebca",
      "metadata": {
        "id": "dd82ebca"
      },
      "source": [
        "# Annotation with Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we are going to see how to generate text and annotations with generative LLMs."
      ],
      "metadata": {
        "id": "wFDXSRAAJhKp"
      },
      "id": "wFDXSRAAJhKp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ùó ACTIVATE THE GPU BY SELECTING RUNTIME IN THE UPPER RIGHT > CONNECT TO RUNTIME > T4 GPU"
      ],
      "metadata": {
        "id": "7FfjwewmJrpN"
      },
      "id": "7FfjwewmJrpN"
    },
    {
      "cell_type": "code",
      "source": [
        "  !pip install transformers accelerate setfit \"pydantic-ai-slim[huggingface]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "jjlIuJ_3KGz8"
      },
      "id": "jjlIuJ_3KGz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ùó RESTART THE NOTEBOOK (DROPDOWN NEXT TO RUN ALL > RESTART SESSION)"
      ],
      "metadata": {
        "id": "qEEOTKYTKIg6"
      },
      "id": "qEEOTKYTKIg6"
    },
    {
      "cell_type": "code",
      "source": [
        "!export HF_TOKEN='insert your hf_token'"
      ],
      "metadata": {
        "id": "boHuFjvpVStz"
      },
      "id": "boHuFjvpVStz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c457798d",
      "metadata": {
        "id": "c457798d"
      },
      "source": [
        "## Generating text with generative models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by simply generating some text using a family of small generative models developed by huggingface."
      ],
      "metadata": {
        "id": "hoh07fK5KfMp"
      },
      "id": "hoh07fK5KfMp"
    },
    {
      "cell_type": "markdown",
      "id": "204c07f3",
      "metadata": {
        "id": "204c07f3"
      },
      "source": [
        "### Simple Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The all-powerful `pipeline` is again the simplest way to get inference running quickly:"
      ],
      "metadata": {
        "id": "SQQgSc39Ku3v"
      },
      "id": "SQQgSc39Ku3v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f84446",
      "metadata": {
        "id": "d2f84446"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "messages = [\n",
        "    \"Let me tell you a story. Once upon a time,\"\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4a63f1",
      "metadata": {
        "id": "eb4a63f1"
      },
      "source": [
        "### Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many applications of LLMs require a chat template. We can use the tokenizer to enforce this template. The template simply indicates which parts of the text are from to the user and which are/should be from the assistant.\n",
        "\n",
        "Remember: LLM chats are just roleplay with special tokens!"
      ],
      "metadata": {
        "id": "MAUcJkbhLE2e"
      },
      "id": "MAUcJkbhLE2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bde39a",
      "metadata": {
        "id": "83bde39a"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "tokenized_chat"
      ],
      "metadata": {
        "id": "f_NxCJ7oMdAR"
      },
      "id": "f_NxCJ7oMdAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this context, it is helpful to add the generation prompt indicating that the text should be generated in the role of the assistant. Otherwise, the model might generate more text as the user instead ([more](https://huggingface.co/docs/transformers/en/chat_templating?template=Mistral#addgenerationprompt))."
      ],
      "metadata": {
        "id": "bFE_XaCbM-P4"
      },
      "id": "bFE_XaCbM-P4"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ],
      "metadata": {
        "id": "-JiKRJGvMy9t"
      },
      "id": "-JiKRJGvMy9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_chat"
      ],
      "metadata": {
        "id": "CN3k4tGlM2sR"
      },
      "id": "CN3k4tGlM2sR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ],
      "metadata": {
        "id": "cMN09_i_MAGT"
      },
      "id": "cMN09_i_MAGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "zhkjNVKNN5qm"
      },
      "id": "zhkjNVKNN5qm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=30) # note the max_new_tokens parameter\n",
        "print(tokenizer.decode(outputs[0])) # note that the entire conversation is returned, including the system prompt."
      ],
      "metadata": {
        "id": "b0TkDgyfLXl6"
      },
      "id": "b0TkDgyfLXl6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7a0c50f5",
      "metadata": {
        "id": "7a0c50f5"
      },
      "source": [
        "### Zero-shot prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get proper annotations from our model, we can simply ask the model to generate the relevant outputs. This is as simple as writing a prompt. Remember the best practices we discussed earlier today."
      ],
      "metadata": {
        "id": "L29OwQklLaN7"
      },
      "id": "L29OwQklLaN7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad546d0",
      "metadata": {
        "id": "dad546d0"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"}\n",
        "    ],\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about AI or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Text: \"The weather is horrible today!\"\n",
        "    \"\"\"}]\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))\n",
        "print(tokenizer.decode(outputs[1][inputs['input_ids'].shape[1]:]))"
      ],
      "metadata": {
        "id": "9LqPh91EPVQ6"
      },
      "id": "9LqPh91EPVQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8459dd43",
      "metadata": {
        "id": "8459dd43"
      },
      "source": [
        "### Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6eb633",
      "metadata": {
        "id": "6a6eb633"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [{\"role\": \"user\", \"content\": \"\"\"You are an expert annotator with years of experience annotating social science data.\n",
        "    Your main task is to annotate whether the following text is about artificial intelligence or not.\n",
        "    Respond ONLY with the label \"AI\" or \"NOT AI\". Do NOT provide an explanation\n",
        "\n",
        "    Example:\n",
        "    \"SmolLM is a pretty impressive model!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "    Example:\n",
        "    \"The weather is horrible today!\"\n",
        "    \"\"\"},\n",
        "     {\"role\": \"assistant\", \"content\": \"NOT AI\"},\n",
        "     {\"role\": \"user\", \"content\": \"\"\"\n",
        "     Text: \"The impact of the new wave on automation on the labour market is not yet clear.\"\n",
        "     \"\"\"}\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "  padding=True,\n",
        "\treturn_dict=True, # retains attention mask\n",
        "\treturn_tensors=\"pt\", # returns tensors\n",
        ").to(model.device) # more efficient to put on device"
      ],
      "metadata": {
        "id": "5gXK7SScR-MQ"
      },
      "id": "5gXK7SScR-MQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))"
      ],
      "metadata": {
        "id": "WIT81DDTR_t7"
      },
      "id": "WIT81DDTR_t7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "40e73037",
      "metadata": {
        "id": "40e73037"
      },
      "source": [
        "### Dynamic Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df6379f",
      "metadata": {
        "id": "7df6379f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b752e701",
      "metadata": {
        "id": "b752e701"
      },
      "source": [
        "### BONUS: Setfit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9464ac08",
      "metadata": {
        "id": "9464ac08"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3a338354",
      "metadata": {
        "id": "3a338354"
      },
      "source": [
        "## Controlling model output with `pydantic`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I9-4NMyPUyaW"
      },
      "id": "I9-4NMyPUyaW"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "from pydantic_ai import Agent\n",
        "\n",
        "agent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n",
        "\n",
        "\n",
        "class CityLocation(BaseModel):\n",
        "    city: str\n",
        "    country: str\n",
        "\n",
        "\n",
        "agent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\n",
        "result = agent.run_sync('Where were the olympics held in 2012?')\n",
        "print(result.output)\n",
        "#> city='London' country='United Kingdom'\n",
        "print(result.usage())\n",
        "#> RunUsage(input_tokens=57, output_tokens=8, requests=1)"
      ],
      "metadata": {
        "id": "FdKwU1CQUXUv"
      },
      "id": "FdKwU1CQUXUv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc17c104",
      "metadata": {
        "id": "dc17c104"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define what we want to extract\n",
        "class Sentiment(BaseModel):\n",
        "    \"\"\"Simple sentiment analysis\"\"\"\n",
        "    sentiment: str = Field(description=\"Is this POSITIVE, NEGATIVE, or NEUTRAL?\")\n",
        "    confidence: float = Field(description=\"How confident are you? (0.0 to 1.0)\")\n",
        "\n",
        "# Test texts to analyze\n",
        "test_texts = [\n",
        "    \"I absolutely love this new policy! It will help so many families.\",\n",
        "    \"This decision is terrible and will hurt our community.\",\n",
        "    \"The committee met yesterday to discuss the budget proposal.\"\n",
        "]\n",
        "\n",
        "print(\"Analyzing sentiments:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for text in test_texts:\n",
        "    # Ask AI to analyze sentiment\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Classify sentiment as POSITIVE, NEGATIVE, or NEUTRAL.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }\n",
        "        ],\n",
        "        add_generation_prompt=True,\n",
        "        padding=True,\n",
        "        return_dict=True, # retains attention mask\n",
        "        return_tensors=\"pt\", # returns tensors\n",
        "      ).to(model.device)\n",
        "\n",
        "    response = model.generate(**inputs, max_new_tokens=3)\n",
        "\n",
        "    # Get the result\n",
        "    result = Sentiment.model_validate_json(tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:]))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    print(f\"‚Üí Sentiment: {result.sentiment}\")\n",
        "    print(f\"‚Üí Confidence: {result.confidence:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bba09",
      "metadata": {
        "id": "d81bba09"
      },
      "source": [
        "## Training Encoders with Synthetic Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80479c8f",
      "metadata": {
        "id": "80479c8f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c38ab856",
      "metadata": {
        "id": "c38ab856"
      },
      "outputs": [],
      "source": [
        "# NLP Workshop: LLM Inference, Text Similarity, and Model Training\n",
        "# A hands-on introduction to modern NLP techniques with Hugging Face\n",
        "\n",
        "\"\"\"\n",
        "Workshop Outline:\n",
        "1. LLM Inference with Zero-shot and Few-shot Prompting\n",
        "2. Text Similarity using Transformer Embeddings\n",
        "3. Dynamic Few-shot Prompting\n",
        "4. Training BERT on Synthetic LLM Labels\n",
        "\n",
        "Prerequisites: Basic Python knowledge, familiarity with transformers concept\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages (run in terminal or uncomment below)\n",
        "# !pip install transformers torch sentence-transformers datasets scikit-learn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AutoModel,\n",
        "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "print(\"Setup complete! üöÄ\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: LLM INFERENCE WITH ZERO-SHOT AND FEW-SHOT PROMPTING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 1: LLM INFERENCE - ZERO-SHOT AND FEW-SHOT PROMPTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load a smaller language model for demonstration\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"Generate text using the loaded model\"\"\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ZERO-SHOT PROMPTING EXAMPLE\n",
        "print(\"\\nüìù Zero-shot Prompting Example:\")\n",
        "zero_shot_prompt = \"Classify the sentiment of this text as positive, negative, or neutral: 'I love this product!' Answer:\"\n",
        "\n",
        "result = generate_text(zero_shot_prompt, max_length=50)\n",
        "print(f\"Prompt: {zero_shot_prompt}\")\n",
        "print(f\"Result: {result}\")\n",
        "\n",
        "# STUDENT INTERACTION 1\n",
        "print(\"\\nü§î STUDENT EXERCISE 1:\")\n",
        "print(\"Try creating your own zero-shot prompt for a different task (e.g., topic classification, question answering)\")\n",
        "print(\"Modify the 'your_prompt' variable below and run the cell!\")\n",
        "\n",
        "# TODO: Students fill this in\n",
        "your_prompt = \"Classify this email as spam or not spam: 'Get rich quick! Click here now!' Answer:\"\n",
        "your_result = generate_text(your_prompt, max_length=50)\n",
        "print(f\"Your result: {your_result}\")\n",
        "\n",
        "# FEW-SHOT PROMPTING EXAMPLE\n",
        "print(\"\\nüìù Few-shot Prompting Example:\")\n",
        "few_shot_prompt = \"\"\"\n",
        "Classify sentiment as positive, negative, or neutral:\n",
        "\n",
        "Text: \"This movie was amazing!\"\n",
        "Sentiment: positive\n",
        "\n",
        "Text: \"I hated every minute of it.\"\n",
        "Sentiment: negative\n",
        "\n",
        "Text: \"It was okay, nothing special.\"\n",
        "Sentiment: neutral\n",
        "\n",
        "Text: \"Best purchase ever!\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "result = generate_text(few_shot_prompt, max_length=60)\n",
        "print(f\"Few-shot result: {result}\")\n",
        "\n",
        "# STUDENT INTERACTION 2\n",
        "print(\"\\nü§î STUDENT EXERCISE 2:\")\n",
        "print(\"Compare zero-shot vs few-shot results. Which performs better? Why?\")\n",
        "print(\"Try adding more examples to the few-shot prompt and observe the difference.\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: TEXT SIMILARITY USING TRANSFORMER EMBEDDINGS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 2: TEXT SIMILARITY WITH TRANSFORMER EMBEDDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load sentence transformer model\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
        "    embeddings = sentence_model.encode([text1, text2])\n",
        "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Example texts for similarity comparison\n",
        "texts = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"A feline rested on the rug.\",\n",
        "    \"Dogs are great pets.\",\n",
        "    \"I love pizza and pasta.\",\n",
        "    \"Italian food is delicious.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìä Text Similarity Matrix:\")\n",
        "print(\"Comparing different text pairs:\")\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    for j in range(i+1, len(texts)):\n",
        "        similarity = calculate_similarity(texts[i], texts[j])\n",
        "        print(f\"'{texts[i][:30]}...' vs '{texts[j][:30]}...': {similarity:.3f}\")\n",
        "\n",
        "# STUDENT INTERACTION 3\n",
        "print(\"\\nü§î STUDENT EXERCISE 3:\")\n",
        "print(\"Add your own texts to the list and see how they compare!\")\n",
        "print(\"Which pairs have the highest/lowest similarity? Does it make sense?\")\n",
        "\n",
        "# TODO: Students add their texts here\n",
        "student_texts = [\n",
        "    \"Your text 1 here\",\n",
        "    \"Your text 2 here\",\n",
        "    # Add more texts...\n",
        "]\n",
        "\n",
        "# Semantic search example\n",
        "def semantic_search(query, documents, top_k=3):\n",
        "    \"\"\"Find most similar documents to a query\"\"\"\n",
        "    query_embedding = sentence_model.encode([query])\n",
        "    doc_embeddings = sentence_model.encode(documents)\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            'document': documents[idx],\n",
        "            'similarity': similarities[idx]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Example semantic search\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "    \"Natural language processing helps computers understand text.\",\n",
        "    \"Computer vision enables machines to interpret visual information.\",\n",
        "    \"Reinforcement learning trains agents through trial and error.\"\n",
        "]\n",
        "\n",
        "query = \"How do computers understand language?\"\n",
        "search_results = semantic_search(query, documents)\n",
        "\n",
        "print(f\"\\nüîç Semantic Search Results for: '{query}'\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"{i}. (Score: {result['similarity']:.3f}) {result['document']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: DYNAMIC FEW-SHOT PROMPTING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 3: DYNAMIC FEW-SHOT PROMPTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DynamicFewShotPrompter:\n",
        "    def __init__(self, examples, sentence_model):\n",
        "        self.examples = examples\n",
        "        self.sentence_model = sentence_model\n",
        "\n",
        "    def get_relevant_examples(self, query, k=3):\n",
        "        \"\"\"Retrieve k most similar examples to the query\"\"\"\n",
        "        query_embedding = self.sentence_model.encode([query])\n",
        "        example_texts = [ex['input'] for ex in self.examples]\n",
        "        example_embeddings = self.sentence_model.encode(example_texts)\n",
        "\n",
        "        similarities = cosine_similarity(query_embedding, example_embeddings)[0]\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        return [self.examples[idx] for idx in top_indices]\n",
        "\n",
        "    def create_prompt(self, query, task_description, k=3):\n",
        "        \"\"\"Create a dynamic few-shot prompt\"\"\"\n",
        "        relevant_examples = self.get_relevant_examples(query, k)\n",
        "\n",
        "        prompt = f\"{task_description}\\n\\n\"\n",
        "\n",
        "        for ex in relevant_examples:\n",
        "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "\n",
        "        prompt += f\"Input: {query}\\nOutput:\"\n",
        "        return prompt\n",
        "\n",
        "# Example dataset for sentiment analysis\n",
        "sentiment_examples = [\n",
        "    {\"input\": \"I love this movie!\", \"output\": \"positive\"},\n",
        "    {\"input\": \"This food tastes terrible\", \"output\": \"negative\"},\n",
        "    {\"input\": \"The weather is nice today\", \"output\": \"positive\"},\n",
        "    {\"input\": \"I'm feeling sad\", \"output\": \"negative\"},\n",
        "    {\"input\": \"This book is okay\", \"output\": \"neutral\"},\n",
        "    {\"input\": \"Amazing service at this restaurant\", \"output\": \"positive\"},\n",
        "    {\"input\": \"The product broke after one day\", \"output\": \"negative\"},\n",
        "    {\"input\": \"Not bad, could be better\", \"output\": \"neutral\"},\n",
        "    {\"input\": \"Absolutely fantastic experience\", \"output\": \"positive\"},\n",
        "    {\"input\": \"Waste of money\", \"output\": \"negative\"}\n",
        "]\n",
        "\n",
        "# Initialize dynamic prompter\n",
        "prompter = DynamicFewShotPrompter(sentiment_examples, sentence_model)\n",
        "\n",
        "# Test dynamic prompting\n",
        "test_query = \"This pizza is incredibly delicious\"\n",
        "dynamic_prompt = prompter.create_prompt(\n",
        "    test_query,\n",
        "    \"Classify the sentiment of the following text as positive, negative, or neutral:\",\n",
        "    k=3\n",
        ")\n",
        "\n",
        "print(f\"üìù Dynamic Few-shot Prompt for: '{test_query}'\")\n",
        "print(f\"\\n{dynamic_prompt}\")\n",
        "\n",
        "# Compare with random few-shot\n",
        "random_examples = random.sample(sentiment_examples, 3)\n",
        "random_prompt = \"Classify the sentiment of the following text as positive, negative, or neutral:\\n\\n\"\n",
        "for ex in random_examples:\n",
        "    random_prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "random_prompt += f\"Input: {test_query}\\nOutput:\"\n",
        "\n",
        "print(f\"\\nüìù Random Few-shot Prompt (for comparison):\")\n",
        "print(f\"\\n{random_prompt}\")\n",
        "\n",
        "# STUDENT INTERACTION 4\n",
        "print(\"\\nü§î STUDENT EXERCISE 4:\")\n",
        "print(\"Try different queries and compare dynamic vs random few-shot selection.\")\n",
        "print(\"Do you notice any differences in the selected examples?\")\n",
        "\n",
        "# TODO: Students test with their own queries\n",
        "student_query = \"I'm not sure how I feel about this\"\n",
        "student_prompt = prompter.create_prompt(student_query, \"Classify sentiment:\", k=3)\n",
        "print(f\"\\nYour dynamic prompt preview:\\n{student_prompt[:200]}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: TRAINING BERT ON SYNTHETIC LLM LABELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 4: TRAINING BERT ON SYNTHETIC LLM LABELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create synthetic dataset (simulating LLM-generated labels)\n",
        "synthetic_data = [\n",
        "    {\"text\": \"I absolutely love this product\", \"label\": 1},\n",
        "    {\"text\": \"This is terrible quality\", \"label\": 0},\n",
        "    {\"text\": \"Not sure about this purchase\", \"label\": 2},\n",
        "    {\"text\": \"Best decision ever\", \"label\": 1},\n",
        "    {\"text\": \"Completely disappointed\", \"label\": 0},\n",
        "    {\"text\": \"It's alright, nothing special\", \"label\": 2},\n",
        "    {\"text\": \"Highly recommend to everyone\", \"label\": 1},\n",
        "    {\"text\": \"Worst experience ever\", \"label\": 0},\n",
        "    {\"text\": \"Could be better or worse\", \"label\": 2},\n",
        "    {\"text\": \"Exceeded my expectations\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Convert to dataset format\n",
        "df = pd.DataFrame(synthetic_data)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"üìä Synthetic Dataset Overview:\")\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df['label'].value_counts().sort_index())\n",
        "print(f\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Load BERT model for sequence classification\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3  # positive, negative, neutral\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the input texts\"\"\"\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split into train/test (small dataset, so simple split)\n",
        "train_size = int(0.8 * len(tokenized_dataset))\n",
        "train_dataset = tokenized_dataset.select(range(train_size))\n",
        "test_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
        "\n",
        "print(f\"\\nüìö Dataset Split:\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Define training arguments (simplified for workshop)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Metric computation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nüèãÔ∏è Starting BERT Training...\")\n",
        "print(\"Note: This is a simplified example. In practice, you'd use larger datasets!\")\n",
        "\n",
        "# Train the model (commented out to avoid long execution time in demo)\n",
        "# trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training setup complete!\")\n",
        "print(\"\\nIn a real scenario, you would:\")\n",
        "print(\"1. Generate more synthetic labels using an LLM\")\n",
        "print(\"2. Clean and validate the synthetic data\")\n",
        "print(\"3. Train on a larger dataset\")\n",
        "print(\"4. Evaluate on human-labeled test data\")\n",
        "print(\"5. Compare performance with the original LLM\")\n",
        "\n",
        "# STUDENT INTERACTION 5\n",
        "print(\"\\nü§î FINAL STUDENT EXERCISE:\")\n",
        "print(\"Discussion Questions:\")\n",
        "print(\"1. What are the advantages of training BERT on LLM-generated labels?\")\n",
        "print(\"2. What potential issues should we watch out for?\")\n",
        "print(\"3. How would you validate that the synthetic labels are good quality?\")\n",
        "print(\"4. In what scenarios would this approach be most useful?\")\n",
        "\n",
        "# Quick inference example (without training)\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Quick inference example\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "\n",
        "    labels = {0: \"negative\", 1: \"positive\", 2: \"neutral\"}\n",
        "    confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    return labels[predicted_class], confidence\n",
        "\n",
        "# Test the model (before training, so results will be random)\n",
        "test_text = \"I think this workshop was helpful\"\n",
        "prediction, confidence = predict_sentiment(test_text)\n",
        "print(f\"\\nüîÆ Model Prediction (before fine-tuning):\")\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"Prediction: {prediction} (confidence: {confidence:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ WORKSHOP COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"Key Takeaways:\")\n",
        "print(\"‚úÖ Zero-shot vs Few-shot prompting strategies\")\n",
        "print(\"‚úÖ Text similarity with transformer embeddings\")\n",
        "print(\"‚úÖ Dynamic example selection for better prompting\")\n",
        "print(\"‚úÖ Training smaller models on LLM-generated data\")\n",
        "print(\"\\nNext steps: Experiment with larger datasets and different model architectures!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}