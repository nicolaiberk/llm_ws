{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/04b_finetuning_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9a8a5f",
      "metadata": {
        "id": "0b9a8a5f"
      },
      "source": [
        "# Transformers: Fine-tune your own BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d217fee9",
      "metadata": {
        "id": "d217fee9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69356d21",
      "metadata": {
        "id": "69356d21"
      },
      "source": [
        "In this notebook, we will\n",
        "\n",
        "1) look at more functions from the ðŸ¤— `transformers` library\n",
        "2) learn how to work with ðŸ¤— `datasets`\n",
        "3) fine-tune a pre-trained BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774609ed",
      "metadata": {
        "id": "774609ed"
      },
      "source": [
        "## What happens in `pipeline()`?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541f9107",
      "metadata": {
        "id": "541f9107"
      },
      "source": [
        "*This section is based on [Chapter 2](https://huggingface.co/learn/llm-course/chapter2/2) of the ðŸ¤— LLM Course.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b029cd45",
      "metadata": {
        "id": "b029cd45"
      },
      "source": [
        "You are already familiar with the `pipeline()` method from our preceding notebook. Here is an example to jog your memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aac581b",
      "metadata": {
        "id": "9aac581b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"My research is going to be whole different ballgame using LLMs!\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6e971b",
      "metadata": {
        "id": "ff6e971b"
      },
      "source": [
        "Alright, so this little function takes text inputs and transforms them into label predictions. But since transformers cannot simply be fit on the raw texts, several things have to happen in the pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33d7e00",
      "metadata": {
        "id": "c33d7e00"
      },
      "source": [
        "![Visualization from Chapter 2 of teh Huggingface Course](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1045cfc4",
      "metadata": {
        "id": "1045cfc4"
      },
      "source": [
        "You already know from our last session how to load tokenizers of specific models and tokenize input texts using `AutoTokenizer()`. Lets's repeat this briefly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6c2b33",
      "metadata": {
        "id": "9c6c2b33"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # this is the default model for sentiment-analysis\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b95143c",
      "metadata": {
        "id": "0b95143c"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    \"My research is going to be whole different ballgame using LLMs!\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6135312",
      "metadata": {
        "id": "b6135312"
      },
      "source": [
        "*Can you explain what the padding and truncation parameters do here? What would the data look like if you would not specify them?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0409afbe",
      "metadata": {
        "id": "0409afbe"
      },
      "source": [
        "Great. So we've already covered the first step! Let's look at how we could load a specific model and generate output embeddings of each text using our token IDs. We start by loading the model from the hub using `AutoModel()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b7fc84",
      "metadata": {
        "id": "f3b7fc84"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6e6a83",
      "metadata": {
        "id": "0c6e6a83"
      },
      "source": [
        "This architecture contains only the base Transformer module: given some inputs, it returns the output embeddings or *hidden states* of the model, which might then be fed into a classification head downstream. We can simply use the assigned object to run the forward pass and return the model embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48cabf9",
      "metadata": {
        "id": "d48cabf9"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fc56b5",
      "metadata": {
        "id": "63fc56b5"
      },
      "source": [
        "*Interpret these dimensions. What does each number describe?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2d2b08",
      "metadata": {
        "id": "fd2d2b08"
      },
      "source": [
        "Now we need to transform these representations into useful classifications. As you heard already earlier today, models 'translate' embeddings into classifications using a **classification head**. Classification heads exist for many different tasks, e.g. question answering or token classification. We are interested in classifying our entire sequences. To do so, we load the model with a classification head for sequence classification using `AutoModelForSequenceClassification` instead of simply `AutoModel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f48d98a",
      "metadata": {
        "id": "2f48d98a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e08a98",
      "metadata": {
        "id": "94e08a98"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa2da26",
      "metadata": {
        "id": "9fa2da26"
      },
      "source": [
        "Instead of outputting the embeddings, this model contains a classification head which transforms the original, high-dimensional representation into a vector containing two values (one for each label). The values we get as output from our model donâ€™t necessarily make sense by themselves - they are so-called logits. We can transform them to values bounded between 0 and 1 using the `softmax` function you already saw earlier today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993aa18e",
      "metadata": {
        "id": "993aa18e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions.round(decimals=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff52c686",
      "metadata": {
        "id": "ff52c686"
      },
      "source": [
        "To see which label corresponds to which prediction, you can check the model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7243f2",
      "metadata": {
        "id": "8c7243f2"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b3d5cd",
      "metadata": {
        "id": "a4b3d5cd"
      },
      "source": [
        "## Working with ðŸ¤— `datasets`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c35c99bc",
      "metadata": {
        "id": "c35c99bc"
      },
      "source": [
        "Before we get started with training our own models, we need to understand how to provide useful inputs to our models. Huggingface provides a library called `datasets` to make our life a little easier.\n",
        "\n",
        "The most basic function in the datasets library is probably `load_dataset`. Using this function, we can load an annotated dataset of synthetically generated phone calls, annotated for whether a given conversation is a scam or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df783bfb",
      "metadata": {
        "id": "df783bfb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"BothBosu/scam-dialogue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f3e4c9",
      "metadata": {
        "id": "02f3e4c9"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01b681c",
      "metadata": {
        "id": "a01b681c"
      },
      "source": [
        "As you can see, we get a dictionary (specifically, a `DatasetDict`) containing two parts of this dataset: a train set (referred to as a 'split) in the datasets lingo; and a test set. You can directly dowload either by specifying the split you are interested in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb53d7ea",
      "metadata": {
        "id": "bb53d7ea"
      },
      "outputs": [],
      "source": [
        "dataset_train = load_dataset(\"BothBosu/scam-dialogue\", split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05cee319",
      "metadata": {
        "id": "05cee319"
      },
      "outputs": [],
      "source": [
        "dataset_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b07631c",
      "metadata": {
        "id": "9b07631c"
      },
      "source": [
        "We have three features: dialogue (containing the text of the conversation), the type of the conversation (which we will ignore here), and the label, that is the outcome of interest - whether the conversation was a scam call (1) or not (0). *You can check the [dataset card](https://huggingface.co/datasets/BothBosu/scam-dialogue) for more information.*\n",
        "\n",
        "You can inspect observations in the data simply by subsetting to the row of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ddc54d",
      "metadata": {
        "id": "04ddc54d"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8421a1",
      "metadata": {
        "id": "da8421a1"
      },
      "source": [
        "Another neat function is `train_test_split`: it divides the dataset into a train and a test set. We can use it here to subdivide our trainset into a validation and a training set. It is useful to set a seed for these operations, in order to make the process reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a28ca07b",
      "metadata": {
        "id": "a28ca07b"
      },
      "outputs": [],
      "source": [
        "train_val_set = dataset['train'].train_test_split(test_size=0.1, shuffle=True, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a3da77",
      "metadata": {
        "id": "b0a3da77"
      },
      "outputs": [],
      "source": [
        "dataset['train'] = train_val_set['train']\n",
        "dataset['validation'] = train_val_set['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2b5020",
      "metadata": {
        "id": "3c2b5020"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4e61e0",
      "metadata": {
        "id": "db4e61e0"
      },
      "source": [
        "Let's prepare the dataset by tokenizing the conversations. The datasets library offers a neat function called `map` to simply and quickly apply any transformation to our data. We start by loading the tokenizer of a pre-trained bert model and defining the function we want to apply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d941fc7",
      "metadata": {
        "id": "1d941fc7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode(examples):\n",
        "    return tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f814b26",
      "metadata": {
        "id": "8f814b26"
      },
      "source": [
        "We can then use `map` to encode each dataset in the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26830692",
      "metadata": {
        "id": "26830692"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(encode, batched=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8ce7449",
      "metadata": {
        "id": "a8ce7449"
      },
      "source": [
        "As you can see, we have added three columns to our dataset, containing the tokenizer output for each conversation. Note that the logic of map allows you to apply any transformation you want - you simply need to change the supplied function.\n",
        "\n",
        "Our model will expect a variable called 'labels' as outcome. So let's rename our variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848ad923",
      "metadata": {
        "id": "848ad923"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962ef95c",
      "metadata": {
        "id": "962ef95c"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93954d5b",
      "metadata": {
        "id": "93954d5b"
      },
      "source": [
        "Lastly, we select the columns of interest using `select_columns`.\n",
        "\n",
        "*For a full overview of pre-processing functions, check out the [datasets documentation](https://huggingface.co/docs/datasets/en/process).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b1908a",
      "metadata": {
        "id": "87b1908a"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.select_columns([\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2f0e08",
      "metadata": {
        "id": "4d2f0e08"
      },
      "source": [
        "*If you want to learn more about ðŸ¤— `datasets` you can check out the [`datasets` tutorial](https://huggingface.co/docs/datasets/en/quickstart) and the [chapter on `datasets` in the ðŸ¤— LLM course](https://huggingface.co/learn/llm-course/chapter5/1).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93bd4d3",
      "metadata": {
        "id": "b93bd4d3"
      },
      "source": [
        "### Using your own data as a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3bdf8df",
      "metadata": {
        "id": "d3bdf8df"
      },
      "source": [
        "datasets also offers helpful functions to load your own data. There are [many functions](https://huggingface.co/docs/datasets/en/loading) to do this, including loading directly from disk. However, especially if you need to prepare the dataset beforehand, it is usually simplest to work with `pandas` and then transform this dataframe to a datasets dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b2a74f",
      "metadata": {
        "id": "a3b2a74f"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
        "tmp_dataset = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83ee7f1",
      "metadata": {
        "id": "b83ee7f1"
      },
      "source": [
        "## Fine-tuning a Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e22c21",
      "metadata": {
        "id": "a3e22c21"
      },
      "source": [
        "Now you should be prepared to understand how to fine-tune your own model. We start by loading a model with the appropriate classification head and defining the number of outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415f302e",
      "metadata": {
        "id": "415f302e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ba81c3",
      "metadata": {
        "id": "c1ba81c3"
      },
      "outputs": [],
      "source": [
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d643f685",
      "metadata": {
        "id": "d643f685"
      },
      "source": [
        "Huggingface offers the `Trainer` function to make training as simple as possible for us. All we need to do is to define some training arguments, supply the training and (optionally) validation data. We can specify many different hyperparameters in the training arguments, but possibly the most important one is the number of epochs - that is, full runs through the training data - you want the model to train. We also specify a validation strategy (once after every epoch) to assess how well our model predicts the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab381d4",
      "metadata": {
        "id": "1ab381d4"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"test-trainer\",\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68a45d3",
      "metadata": {
        "id": "a68a45d3"
      },
      "source": [
        "We should also specify an evaluation metric to understand how well our model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7716601f",
      "metadata": {
        "id": "7716601f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def acc_prec_rec_f1(predictions, references):\n",
        "    cm = pd.crosstab(predictions, references)\n",
        "    accuracy = np.diag(cm).sum() / cm.sum().sum()\n",
        "\n",
        "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return acc_prec_rec_f1(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662c59b9",
      "metadata": {
        "id": "662c59b9"
      },
      "source": [
        "Quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801580aa",
      "metadata": {
        "id": "801580aa"
      },
      "outputs": [],
      "source": [
        "predictions = [0,1,0,0,1]\n",
        "references =  [0,1,0,1,1]\n",
        "\n",
        "acc_prec_rec_f1(predictions, references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee9265f",
      "metadata": {
        "id": "9ee9265f"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46828f2",
      "metadata": {
        "id": "b46828f2"
      },
      "source": [
        "You then train the model simply by running `trainer.train()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6c72ed",
      "metadata": {
        "id": "3d6c72ed"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdbb970e",
      "metadata": {
        "id": "fdbb970e"
      },
      "source": [
        "*What is happening during `trainer.train()`?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "873bdfa1",
      "metadata": {
        "id": "873bdfa1"
      },
      "source": [
        "Best to save the trained model directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059c35e4",
      "metadata": {
        "id": "059c35e4"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"my_first_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7f420b",
      "metadata": {
        "id": "de7f420b"
      },
      "source": [
        "You can generate predictions of the trained model using `model.predict()` now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc39f4ce",
      "metadata": {
        "id": "bc39f4ce"
      },
      "outputs": [],
      "source": [
        "preds = trainer.predict(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdd6efa",
      "metadata": {
        "id": "3cdd6efa"
      },
      "outputs": [],
      "source": [
        "preds.predictions[0] # again, we get logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d078b97c",
      "metadata": {
        "id": "d078b97c"
      },
      "outputs": [],
      "source": [
        "## transform logits to labels\n",
        "pred_labels = np.argmax(preds.predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e259acfd",
      "metadata": {
        "id": "e259acfd"
      },
      "outputs": [],
      "source": [
        "pred_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd30ab8",
      "metadata": {
        "id": "4dd30ab8"
      },
      "source": [
        "Finally, we evaluate the model on our testset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc024470",
      "metadata": {
        "id": "bc024470"
      },
      "outputs": [],
      "source": [
        "## evaluate\n",
        "acc_prec_rec_f1(pred_labels, dataset['test']['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bbbabc",
      "metadata": {
        "id": "e1bbbabc"
      },
      "source": [
        "*Why would it be necessary or useful to use a separate testset here?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19dc99b",
      "metadata": {
        "id": "b19dc99b"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08eeb8e",
      "metadata": {
        "id": "f08eeb8e"
      },
      "source": [
        "TIme to train your own classifier! Look for data of interest, start a new notebook and fine-tune a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dea21d9",
      "metadata": {
        "id": "2dea21d9"
      },
      "source": [
        "1. Start by identifying a reasonably sized dataset (max 100k rows) to use for sequence classification from the [transformers hub](https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3C1K,size_categories:1K%3Cn%3C10K,size_categories:10K%3Cn%3C100K%29&task_categories=task_categories:text-classification&sort=downloads). Load the data. (If you do not know which one to use, I recommend https://huggingface.co/datasets/stanfordnlp/sst2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fab4e87",
      "metadata": {
        "id": "8fab4e87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d9107514",
      "metadata": {
        "id": "d9107514"
      },
      "source": [
        "2. Preprocess the data appropriately. Split the data into train, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "007031de",
      "metadata": {
        "id": "007031de"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "745b5926",
      "metadata": {
        "id": "745b5926"
      },
      "source": [
        "3. Load a 'bert-base-uncased' model and train the model for at least one epoch. Explicitly define some other hyperparameters if you want to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adeeb261",
      "metadata": {
        "id": "adeeb261"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4f05dbcb",
      "metadata": {
        "id": "4f05dbcb"
      },
      "source": [
        "4. Evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860fd4b8",
      "metadata": {
        "id": "860fd4b8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}