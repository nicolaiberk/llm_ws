{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/02b_embeddings_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAqJl9nG8Ifv"
      },
      "source": [
        "# Scaling Word Embeddings & Document Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeiVHlEdyaUK"
      },
      "outputs": [],
      "source": [
        "!pip install gensim # restart after installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz9Zmg7oHC89"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCSzG1XB8WmV"
      },
      "source": [
        "## Calculating Projections and Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwx1A5L_HC9A"
      },
      "outputs": [],
      "source": [
        "## again we load the 100-dimensional GloVe model trained on Wikipedia data\n",
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ6irr5DHC9H"
      },
      "source": [
        "We can calculate the projection of vector a on a given vector b by using the dot product, divided by the magnitude of vector b:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_K3CIfYHC9J"
      },
      "outputs": [],
      "source": [
        "## calculate dot product\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def proj_mag(a, b):\n",
        "    return dot(a, b) / norm(b)\n",
        "\n",
        "proj_mag(wv['nurse'], wv['woman'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HQ_3YV9HC9O"
      },
      "source": [
        "But cosine similarity is a lot easier to interpret, since it is bounded [-1,1]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0YgEU3rG22N"
      },
      "outputs": [],
      "source": [
        "def cos_sim(a, b):\n",
        "    return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "cos_sim(wv['nurse'], wv['woman'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZiV8a_uHC9T"
      },
      "source": [
        "As you have seen in the first tutorial, there is also a function in the `gensim`-package to calculate the cosine similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYBEwP9lF8Wp"
      },
      "outputs": [],
      "source": [
        "wv.similarity('nurse', 'woman')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmGpkFC2HC9W"
      },
      "source": [
        "## Defining Scales\n",
        "\n",
        "An association of a given word might not tell us much, especially because opposite concepts are often located close to each other in the vector space. Rather than the association of two words, we might often be interested in the relative position of a concept on a pre-defined scale. We can create such a scale by subtracting the negative pole of that scale from the positive pole. You have seen this concept being used already when we calculated the word vector for 'berlin' from the vectors for 'paris', 'france', and 'germany'. Try to imagine this subtraction in the vector space.\n",
        "\n",
        "By calculating the magnitude of the projection of a given word to this defined axis, we can identify the position this word has relative to this scale. This visualization from the Kozlowski paper might be helpful to understand this concept:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFJMO8MNHC9X"
      },
      "source": [
        "![](https://www.dropbox.com/scl/fi/fy9ihaxiwiql2c0s13vjy/projection_example.png?rlkey=aszlo5mp3rdzy4svawug5eybl&dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2jNncmwHC9Y"
      },
      "source": [
        "You can hence create pretty much any scale by defining two poles for it, subtracting the negative from the positive pole, and calculating the magnitude of the projection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv3weEFcbbAq"
      },
      "outputs": [],
      "source": [
        "## simple scaling, projection magnitude\n",
        "fr_de_scale = wv['french'] - wv['german']\n",
        "print(\"Frenchness of Paris: \", proj_mag(wv['paris'], fr_de_scale))\n",
        "print(\"Frenchness of Lausanne: \", proj_mag(wv['lausanne'], fr_de_scale))\n",
        "print(\"Frenchness of Bern: \", proj_mag(wv['bern'], fr_de_scale))\n",
        "print(\"Frenchness of Berlin: \", proj_mag(wv['berlin'], fr_de_scale))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZxpO-MnHC9Z"
      },
      "source": [
        "As mentioned, the boundedness of the cosine similarity [-1,1] makes it a lot more interpretable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSM5OxvSHC9Z"
      },
      "outputs": [],
      "source": [
        "## simple scaling, cosine similarity\n",
        "fr_de_scale = wv['french'] - wv['german']\n",
        "print(\"Frenchness of Paris: \", cos_sim(wv['paris'], fr_de_scale))\n",
        "print(\"Frenchness of Lausanne: \", cos_sim(wv['lausanne'], fr_de_scale))\n",
        "print(\"Frenchness of Bern: \", cos_sim(wv['bern'], fr_de_scale))\n",
        "print(\"Frenchness of Berlin: \", cos_sim(wv['berlin'], fr_de_scale))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQuthQx5HC9a"
      },
      "source": [
        "In practice, it is better to create a scale from an averaged vector of multiple pole words (often referred to as the 'centroid'). To do so, we define our pole words and average across their vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3FoA4X5HC9c"
      },
      "outputs": [],
      "source": [
        "## start by defining dictionaries (usually larger than 3-4 words)\n",
        "rich_words = ['rich', 'wealthy', 'wealth', 'billionaire']\n",
        "poor_words = ['poor', 'broke', 'poverty', 'beggar']\n",
        "\n",
        "female_words = ['female', 'woman', 'feminine']\n",
        "male_words = ['male', 'man', 'masculine']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6_JQDF8EbFV"
      },
      "outputs": [],
      "source": [
        "## pro tip: you can extend your dictionaries with embeddings!\n",
        "wv.most_similar(positive=rich_words, topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoDWcLvEDWdq"
      },
      "outputs": [],
      "source": [
        "## create mean vectors for your concepts of interest\n",
        "rich_vec = np.mean(wv[rich_words], axis = 0)\n",
        "poor_vec = np.mean(wv[poor_words], axis = 0)\n",
        "\n",
        "female_vec = np.mean(wv[female_words], axis = 0)\n",
        "male_vec = np.mean(wv[male_words], axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBid0LNBHC9h"
      },
      "source": [
        "Try to imagine what these vectors look like in the vector space, relative to the dictionary terms. These vectors can already be informative in themselves, given that they are designed to represent the essence of a concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjqHe4sYGSMV"
      },
      "outputs": [],
      "source": [
        "cos_sim(wv['nurse'], female_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFLIqo7hIzuf"
      },
      "outputs": [],
      "source": [
        "cos_sim(wv['nurse'], male_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T3ovCx4HC9i"
      },
      "source": [
        "And the differences of cosine similarities are often used in research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cmtnR4J8t3p"
      },
      "outputs": [],
      "source": [
        "## cosine similarity difference\n",
        "nurse_mf_cos = cos_sim(wv['nurse'], female_vec) - cos_sim(wv['nurse'], male_vec)\n",
        "teacher_mf_cos = cos_sim(wv['teacher'], female_vec) - cos_sim(wv['teacher'], male_vec)\n",
        "lifeguard_mf_cos = cos_sim(wv['lifeguard'], female_vec) - cos_sim(wv['lifeguard'], male_vec)\n",
        "banker_mf_cos = cos_sim(wv['banker'], female_vec) - cos_sim(wv['banker'], male_vec)\n",
        "miner_mf_cos = cos_sim(wv['miner'], female_vec) - cos_sim(wv['miner'], male_vec)\n",
        "\n",
        "print(\"Association of 'nurse' with female (vs. male) terms: \", nurse_mf_cos)\n",
        "print(\"Association of 'teacher' with female (vs. male) terms: \", teacher_mf_cos)\n",
        "print(\"Association of 'lifeguard' with female (vs. male) terms: \", lifeguard_mf_cos)\n",
        "print(\"Association of 'banker' with female (vs. male) terms: \", banker_mf_cos)\n",
        "print(\"Association of 'miner' with female (vs. male) terms: \", miner_mf_cos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_p_xeJuHC9j"
      },
      "source": [
        "However, the neat nature of projections is that they can capture a vectors' position on that very scale. To calculate them, we first need subtract the poles from each other to create the axes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4piSSsqtHC9j"
      },
      "outputs": [],
      "source": [
        "## define axes\n",
        "gender_axis = female_vec - male_vec\n",
        "rich_axis = rich_vec - poor_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYoKDRlbHC9j"
      },
      "source": [
        "... and calculate the cosine similarities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFxRRtnu9QlH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "scaled_occupations = pd.DataFrame(\n",
        "    [['Nurse', 'Teacher', 'Lifeguard', 'Banker', 'Miner'],\n",
        "    [\n",
        "        cos_sim(wv['nurse'], gender_axis),\n",
        "        cos_sim(wv['teacher'], gender_axis),\n",
        "        cos_sim(wv['lifeguard'], gender_axis),\n",
        "        cos_sim(wv['banker'], gender_axis),\n",
        "        cos_sim(wv['miner'], gender_axis)\n",
        "    ],\n",
        "    [\n",
        "        cos_sim(wv['nurse'], rich_axis),\n",
        "        cos_sim(wv['teacher'], rich_axis),\n",
        "        cos_sim(wv['lifeguard'], rich_axis),\n",
        "        cos_sim(wv['banker'], rich_axis),\n",
        "        cos_sim(wv['miner'], rich_axis)\n",
        "    ]]\n",
        ").T.rename(columns={0:'Occupation', 1:'Gender score', 2:'Economic Score'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgrheWyo_hD0"
      },
      "outputs": [],
      "source": [
        "scaled_occupations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09fxfj02_Pw9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(scaled_occupations['Gender score'], scaled_occupations['Economic Score'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Gender Score (Higher = More Female)')\n",
        "plt.ylabel('Economic Score (Higher = Richer)')\n",
        "plt.title('Occupations Scaled by Gender and Status')\n",
        "\n",
        "# Add text labels for each point\n",
        "for i, row in scaled_occupations.iterrows():\n",
        "    plt.text(row['Gender score'], row['Economic Score'], row['Occupation'])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMD-FJQhHC9n"
      },
      "source": [
        "In practice, both approaches (differences in cosine similarities and the cosine similarity relative to a scale) lead to almost identical scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVwfDiYIBDcK"
      },
      "outputs": [],
      "source": [
        "scaled_occupations['cos_dif_gender'] = [nurse_mf_cos, teacher_mf_cos, lifeguard_mf_cos, banker_mf_cos, miner_mf_cos]\n",
        "scaled_occupations[['Gender score', 'cos_dif_gender']].corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9xucGXr8TgJ"
      },
      "source": [
        "## Document Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfzwAqaBjKJ"
      },
      "source": [
        "We'll return to the adapted dataset of dialogue in the Simpsons from [Kaggle](https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset?resource=download&select=simpsons_script_lines.csv). This time, we estimate a model with document embeddings indicating the character who provided the line. This allows us to create semantic vectors for each character in the same space as the word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_2B-td3HC9q"
      },
      "source": [
        "Again, we load and clean our texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Ti4neS80A3"
      },
      "outputs": [],
      "source": [
        "## load data\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('https://www.dropbox.com/scl/fi/n5ffxvm4qyjkp8ws7qgoq/simpsons_script_lines_clean.csv?rlkey=gfliitwgi8cqsjxlcdmmdwtym&dl=1')\n",
        "dataset['cleaned_text'] = dataset['spoken_words'].str.replace('[^a-zA-Z ]','', regex=True) # remove anything that is not a letter or whitespace\n",
        "dataset['cleaned_text'] = dataset['cleaned_text'].str.lower() # lowercase\n",
        "dataset['cleaned_text'] = dataset['cleaned_text'].str.replace(' +', '') # remove multiple whitespaces\n",
        "dataset = dataset[dataset['cleaned_text'] != '']\n",
        "dataset.loc[:,'cleaned_text'] = dataset['cleaned_text'].astype(str)\n",
        "dataset.cleaned_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRn-Pp9OHC9q"
      },
      "source": [
        "We then need to prepare this data for the Doc2Vec-model. Remember that, alongside the vector indicating context words, a vector indicating the document ID is passed. We need to concatenate all texts per speaker in order to create a single 'document' for each speaker. Then, we split those documents into tokens of interest (in our case words, similar to the word2vec model) and use  `gensim`'s `TaggedDocument`-class to pass the information about the speaker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eG7u46H03FGk"
      },
      "outputs": [],
      "source": [
        "## concatenate all cleaned texts from the same speaker and tag documents (this runs a bit)\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "character_texts = list()\n",
        "for speaker in dataset.raw_character_text.unique():\n",
        "    print('Preparing texts from ', speaker, '...')\n",
        "    concat_text = ' '.join(dataset[dataset.raw_character_text == speaker].cleaned_text)\n",
        "    character_texts.append(TaggedDocument(concat_text.split(\" \"), [speaker]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_bYIp2HHC9s"
      },
      "source": [
        "Each document then contains all words uttered from a given character, as well as an indicator for the character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oHRUQYOdFMV0"
      },
      "outputs": [],
      "source": [
        "character_texts[1100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzEef6rHC9s"
      },
      "source": [
        "Time to define our model and train it on these documents!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsHRXAJyGj2o"
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec(vector_size=300, min_count=2, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONn57m0UEDiU"
      },
      "outputs": [],
      "source": [
        "## estimate doc2vec model (this takes some patience)\n",
        "model.build_vocab(character_texts)\n",
        "model.train(character_texts, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_gnd0gHC9t"
      },
      "source": [
        "This results in a model with both word and document embeddings. These can be accessed via `model.dv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLLIKM-4HWtY"
      },
      "outputs": [],
      "source": [
        "model.dv['Homer Simpson'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL5uCXTlHC9u"
      },
      "outputs": [],
      "source": [
        "model.wv['donut'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSFZgu1pHC9u"
      },
      "source": [
        "Crucially, the document embeddings are learned into the same vector space as the word embeddings. This means we can assess which words are associated with this character, meaning which words are similar in meaning as the learned the meaning of the character (note that words are predictive of words used by the character - they might not even be used by the characters themselves)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gt8UqvDHC9w"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(model.dv['Homer Simpson'], topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5mgION6HC9x"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(model.dv['Lisa Simpson'], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWLFLOmHC9x"
      },
      "source": [
        "Of course, we can also plot the positions of different characters in our vector space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnOTAXtCH4rn"
      },
      "outputs": [],
      "source": [
        "## run a dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(model.dv[model.dv.index_to_key])\n",
        "dv_2d = pca.transform(model.dv[model.dv.index_to_key])\n",
        "dv_2d = pd.DataFrame(dv_2d, index = model.dv.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5qthtiKeUu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "plt.scatter(dv_2d[0], dv_2d[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yz64uZgHC9x"
      },
      "source": [
        "And scale them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOf_6HSIH8BM"
      },
      "outputs": [],
      "source": [
        "cool_boring_scale = model.wv['cool'] - model.wv['boring']\n",
        "print(\"Coolness of Homer: \", cos_sim(model.dv['Homer Simpson'], cool_boring_scale))\n",
        "print(\"Coolness of Bart: \", cos_sim(model.dv['Bart Simpson'], cool_boring_scale))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpsmeWweIAOc"
      },
      "outputs": [],
      "source": [
        "## ...or predict vectors for new documents...\n",
        "new_document = [\n",
        "    \"it\", \"wont\", \"last\", \"brothers\", \"and\", \"sisters\", \"are\", \"natural\",\n",
        "    \"enemies\", \"like\", \"englishmen\", \"and\", \"scots\", \"or\", \"welshmen\", \"and\",\n",
        "    \"scots\", \"or\", \"japanese\", \"and\", \"scots\", \"or\", \"scots\", \"and\", \"other\",\n",
        "    \"scots\", \"damn\", \"scots\", \"they\", \"ruined\", \"scotland\"\n",
        "]\n",
        "vector= model.infer_vector(new_document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRJ6Rf8uIeKz"
      },
      "outputs": [],
      "source": [
        "## ...and compare them\n",
        "model.dv.most_similar([vector], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK3ga6eiHC9y"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1) You are interested in the political leaning of the characters. How could you measure their political leaning, based on their language?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p-BEFK34A8FI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXzORYfWHC9y"
      },
      "source": [
        "2) Define dictionaries to measure your concept of interest."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPmaDKF5A8b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYTWE9g4HC9z"
      },
      "source": [
        "3) Create a scale (or if you are very eager, two). Select some characters of interest that you would like to scale. Calculate the cosine similarity of their vectors relative to the scale."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "595Xn6aGA84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkS6-GhvHC9z"
      },
      "source": [
        "4) Create a table of the political leanings of your characters. What do you observe? Who do you think might vote republican?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I73c4sT8A9Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh4mT3BlHC9z"
      },
      "source": [
        "5) Estimate the political leaning of all characters and create a plot of their distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3jaDkiNA9wr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}