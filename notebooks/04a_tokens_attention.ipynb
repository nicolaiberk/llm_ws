{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/04a_tokens_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAqJl9nG8Ifv"
      },
      "source": [
        "# Transformers: Contextualized Embeddings, Tokenization, and Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTgZAY3Os0ru"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers datasets evaluate accelerate\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcrgswLBs0ru"
      },
      "source": [
        "### REMEMBER TO RESTART HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paMNt1ogs0ru"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrj4r5L9F-F"
      },
      "source": [
        "## Contextual Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoMkQloHs0rv"
      },
      "source": [
        "This notebook provides a hands-on introduction to three fundamental concepts in modern NLP:\n",
        "1. **Simple Attention Mechanism** - Understanding how context changes word meaning\n",
        "2. **Subword Tokenization** - How text is broken down for neural models\n",
        "3. **Hugging Face Pipelines** - Quick inference with pre-trained models\n",
        "\n",
        "## 1. Simple Attention Mechanism\n",
        "\n",
        "### Understanding the Problem\n",
        "\n",
        "The word \"flies\" has different meanings in these sentences:\n",
        "- \"Fruit flies like bananas\" - *flies* = insects\n",
        "- \"Time flies like an arrow\" - *flies* = moves quickly\n",
        "\n",
        "Let's see how attention helps disambiguate this!\n",
        "\n",
        "### Setup and Get Word Embeddings\n",
        "\n",
        "We start with some example word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQD-D8501bxe"
      },
      "outputs": [],
      "source": [
        "## word embeddings for the sentences above\n",
        "embeddings = {\n",
        "    'fruit': np.array([0.8, 0.2, 0.1, 0.3]),\n",
        "    'flies': np.array([0.5, 0.5, 0.6, 0.3]),\n",
        "    'like': np.array([0.3, 0.7, 0.4, 0.5]),\n",
        "    'bananas': np.array([0.9, 0.1, 0.2, 0.4]),\n",
        "    'time': np.array([0.1, 0.3, 0.8, 0.7]),\n",
        "    'an': np.array([0.2, 0.4, 0.3, 0.1]),\n",
        "    'arrow': np.array([0.2, 0.4, 0.9, 0.8]),\n",
        "    # Related words for comparison\n",
        "    'insects': np.array([0.7, 0.3, 0.2, 0.1]),\n",
        "    'bugs': np.array([0.6, 0.4, 0.3, 0.2]),\n",
        "    'soars': np.array([0.3, 0.8, 0.7, 0.9]),\n",
        "    'glides': np.array([0.2, 0.7, 0.8, 0.8])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnRhYo-bs0rw"
      },
      "outputs": [],
      "source": [
        "# reduce dimensionality to 2 and plot selected word embeddings\n",
        "\n",
        "## dimensionality reduction using PCA\n",
        "# reduce dimensionality to 2 and plot selected word embeddings\n",
        "\n",
        "## dimensionality reduction using PCA\n",
        "interesting_words = ['fruit', 'flies', 'bananas', 'insects', 'bugs', 'soars', 'glides', 'arrow']\n",
        "interesting_vecs = np.array([embeddings[w] for w in interesting_words])\n",
        "pca = PCA(n_components=2)\n",
        "wv_2d = pca.fit_transform(interesting_vecs)\n",
        "wv_2d = pd.DataFrame(wv_2d, index=interesting_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFrwESGms0rw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(wv_2d[0], wv_2d[1])\n",
        "\n",
        "for i in wv_2d.index:\n",
        "    plt.annotate(i, (wv_2d[0][i], wv_2d[1][i]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Stop here for a second. Attention calculates weights (roughly) by taking the dot product of the token representations. What properties does the dot product have? What does that mean for the attention weights of different context words vis-a-vis 'flies'?*"
      ],
      "metadata": {
        "id": "Esu5oQ_BVRUv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARJtAUAHs0rw"
      },
      "source": [
        "## Calculate Attention\n",
        "\n",
        "For each sentence, we take the dot product of the vector for 'flies' against all others, ensure that the weights sum to 1, multiply with the initial vector, and plot the result. What can you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMDAwPMws0rx"
      },
      "outputs": [],
      "source": [
        "## calculate attention weights\n",
        "sentence_a = [\"time\", \"flies\", \"like\", \"an\", \"arrow\"]\n",
        "sentence_b = [\"fruit\", \"flies\", \"like\", \"bananas\"]\n",
        "\n",
        "scores_a = pd.DataFrame(columns=sentence_a)\n",
        "scores_b = pd.DataFrame(columns=sentence_b)\n",
        "\n",
        "query = embeddings['flies']\n",
        "\n",
        "for key in sentence_a:\n",
        "    score = np.dot(embeddings[key], query)\n",
        "    scores_a.at['flies', key] = score\n",
        "\n",
        "for key in sentence_b:\n",
        "    score = np.dot(embeddings[key], query)\n",
        "    scores_b.at['flies', key] = score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYw8KQeQs0rx"
      },
      "outputs": [],
      "source": [
        "scores_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTAWE7Rfs0rx"
      },
      "outputs": [],
      "source": [
        "scores_a = scores_a.astype(float).values.flatten() ## make list of values\n",
        "scores_b = scores_b.astype(float).values.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3Bpfj7bs0rx"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "# Normalize scores to sum to 1 using softmax\n",
        "norm_scores_a = softmax(scores_a)\n",
        "norm_scores_b = softmax(scores_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB2jN05os0rx"
      },
      "outputs": [],
      "source": [
        "norm_scores_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZnqU6NVs0rx"
      },
      "outputs": [],
      "source": [
        "sum(norm_scores_a) ## double-check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z49gl_q0s0ry"
      },
      "outputs": [],
      "source": [
        "## Calculate Contextualized Vector\n",
        "context_vector_a = np.dot(norm_scores_a, np.array([embeddings[context] for context in sentence_a]))\n",
        "context_vector_b = np.dot(norm_scores_b, np.array([embeddings[context] for context in sentence_b]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITcqS3wws0ry"
      },
      "outputs": [],
      "source": [
        "new_vecs = np.append(interesting_vecs, [context_vector_a, context_vector_b], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GSElWROs0ry"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "wv_2d = pca.fit_transform(new_vecs)\n",
        "wv_2d = pd.DataFrame(wv_2d, index=interesting_words + [\"'flies' (A)\", \"'flies' (B)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34U19EnTs0ry"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(wv_2d[0], wv_2d[1])\n",
        "\n",
        "for i in wv_2d.index:\n",
        "    plt.annotate(i, (wv_2d[0][i], wv_2d[1][i]))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfjFzR2gs0ry"
      },
      "source": [
        "Note that this would not look that great with high-dimensional vectors used in transformers. The large dot products resulting from these vectors create issues. Transformers therefore uses a scaled dot product and learn a weight matrix to learn the query, key, and value vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbKImJL9s0ry"
      },
      "source": [
        "## Tokenization for Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si8C9SOAs0ry"
      },
      "source": [
        "In this part of the tutorial, we are going to explore tokenization in the Huggingface Transformers framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtrJSrx3s0ry"
      },
      "source": [
        "The simplest way to access a tokenizer using the transformers library is the `AutoTokenizer` class. This class automatically provides the right tokenizer for a corresponding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbQenk-Cs0ry"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1PUXfjs0ry"
      },
      "source": [
        "You can encode any input using the tokenizer you just created. It will return a dictionary with three values for each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76EMYYJs0ry"
      },
      "outputs": [],
      "source": [
        "encoded_input = tokenizer(\"Use the GPU!\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wcVvnPls0ry"
      },
      "source": [
        "The `input_ids` provide the ID for a given token. The `attention_mask` indicates which tokens should be attended to. In this case, all tokens are attended to, so the attention mask is all ones. By setting some to zero, you can tell the tokenizer to ignore specific tokens. The `token_type_ids` can be safely ignored for our purposes today (if you are very eager, you can learn more [here](https://huggingface.co/docs/transformers/main/en/glossary#token-type-ids)).\n",
        "\n",
        "You can see that there are plenty more IDs (7) than words (3). Why might that be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndb9DI9ns0ry"
      },
      "outputs": [],
      "source": [
        "len(encoded_input['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h_cQpFps0ry"
      },
      "source": [
        "You can assess the tokenization using the `tokenize` method..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdBGr1wEs0ry"
      },
      "outputs": [],
      "source": [
        "tokenizer.tokenize(\"Use the GPU!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QThvXiOhs0ry"
      },
      "source": [
        "...and map the IDs back to actual text using the `decode` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZJVp8kcs0ry"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5E3lAGVs0rz"
      },
      "source": [
        "Can you remember the use of the [CLS] and [SEP] tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge9pNNqSs0rz"
      },
      "source": [
        "Let's compare this output to another tokenizer, `bert-base-uncased`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_sIqtNts0rz"
      },
      "outputs": [],
      "source": [
        "uncased_tknzr = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "encoded_input = uncased_tknzr(\"Transformers rule!\")\n",
        "print(uncased_tknzr.decode(encoded_input[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8El0b3s0rz"
      },
      "source": [
        "You can also use the method to encode multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTKGfGzPs0rz"
      },
      "outputs": [],
      "source": [
        "batch_sentences = [\n",
        "    'What about second breakfast?',\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    'What about elevensies?'\n",
        "]\n",
        "tokenizer(batch_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaGhWzpjs0rz"
      },
      "source": [
        "Note that you can also explicitly ask for padding tokens in the encoding of batches. This will lead to speed-ups when processing batches (you usually don't have to take care of this yourself)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob4ptVSys0rz"
      },
      "outputs": [],
      "source": [
        "tokenizer(batch_sentences, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLNfnaNDs0rz"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([101, 1327, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLS0-UFWs0rz"
      },
      "source": [
        "Similarly, you can ask for truncation to shorten texts which might be longer than the maximum input of your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn9Apzpns0rz"
      },
      "outputs": [],
      "source": [
        "tokenizer(batch_sentences, padding=True, truncation=True, max_length=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_oOpjQis0rz"
      },
      "source": [
        "The Huggingface library provides the tokenizers alongside the transformer models so you use the right transformer for each model. You can find an extensive explanation of tokenizers [here](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BPBceWfs0r0"
      },
      "source": [
        "## First Inference with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIaeOGSes0r0"
      },
      "source": [
        "The fastest way to run inference is the pipeline function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHLzK1G8s0r0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "feature_extractor = pipeline('feature-extraction', model='bert-base-cased', tokenizer='bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM5SZ4Hgs0r0"
      },
      "outputs": [],
      "source": [
        "features = feature_extractor(\"Transformers are great for NLP tasks!\")\n",
        "len(features[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z5ezKFRs0r0"
      },
      "outputs": [],
      "source": [
        "len(features[0][0])  # Length of the feature vector for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh6qNbDQs0r0"
      },
      "outputs": [],
      "source": [
        "## You can simply call for a different task and the pipeline will replace the classification head\n",
        "classifier = pipeline('sentiment-analysis', model='bert-base-cased', tokenizer='bert-base-cased')\n",
        "classifier(\"Transformers are great for NLP tasks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1hZ83wjs0r0"
      },
      "source": [
        "Particularly useful is zero-shot classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1uWvLWjs0r0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/deberta-v3-base-zeroshot-v2.0\",\n",
        "    tokenizer=\"MoritzLaurer/deberta-v3-base-zeroshot-v2.0\"\n",
        ")\n",
        "\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-GwxoZds0r1"
      },
      "outputs": [],
      "source": [
        "emotions = classifier(\n",
        "    [\"I just cant do this anymore...\", \"I HATE this!\", \"Transformers are amazing!\"],\n",
        "    candidate_labels=[\"happy\", \"angry\", \"sad\"],\n",
        ")\n",
        "[e['labels'][0] for e in emotions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2njJ1ZTXs0r1"
      },
      "source": [
        "You can even process images or audio - the pipeline takes care of the preprocessing! (probably best to restart here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f55k444SUXIN"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqLVKZ4Ys0r1"
      },
      "source": [
        "![](https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ParG4sGs0r1"
      },
      "outputs": [],
      "source": [
        "segments = classifier(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
        "print(segments[0][\"label\"])\n",
        "print(segments[1][\"label\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIHozRy4s0r1"
      },
      "source": [
        "![](https://c8.alamy.com/comp/E75E30/cute-pet-kitten-plays-with-his-deathly-injured-pray-a-magpie-bird-E75E30.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L77JckZds0r1"
      },
      "outputs": [],
      "source": [
        "classifier(\"https://c8.alamy.com/comp/E75E30/cute-pet-kitten-plays-with-his-deathly-injured-pray-a-magpie-bird-E75E30.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-5L4pnHCo4"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VMlpSwFUXIO"
      },
      "source": [
        "0. Explain the logic of contextualized embeddings and their differentiation from classic word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOQrvL-WUXIO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHsh-GbUXIO"
      },
      "source": [
        "1. Consider that the tokenizer we load is identified with a model name - why would this be necessary? What problems would you encounter if that were not the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e30adKAQUXIO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-qs8HlUXIO"
      },
      "source": [
        "2) Take a concept from your own research and define some relevant labels based on the concept. Give three to four examples for each label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WejRHO2-UXIO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KMm6nHdUXIO"
      },
      "source": [
        "3) Annotate the texts using the zero-shot classifier from above. Do you agree with the classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STkqkHKns0r2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COt0hUJs0r2"
      },
      "source": [
        "BONUS: Go to [https://huggingface.co/models](https://huggingface.co/models) and select another task from the Natural Language Processing filters on the left side. Select a model and try to use the pipeline function to run the task. Tip: many models provide the code to implement them on the model page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRUUxvsbs0r2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}