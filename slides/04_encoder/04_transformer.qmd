---
title: "Session 4: The Transformer"
author: "Nicolai Berk"
subtitle: "CUSO WS on Large Language Models"
date: "2025-09-03"
description: "This session covers the concept of embeddings in the context of large language models."
keywords: ["bert", "large language models", "machine learning", "nlp"]
format:
    revealjs:
        theme: simple
        slide-number: true
        toc: false
bibliography: "../references.bib"
---

## Today: Afternoon

- Part I: Advanced NLP
  - Advanced Tokenization
  - Contextualized Embeddings
- Part II: Intro to Transformers
  - The Transformer Architecture
  - The Huggingface Ecosystem
  - Fine-tuning a Transformer Model

# Tokenization

## Issues with simple word tokenization

<br>

### Consider the following words:

"national", "nationalism", "nationalist", "nationalize"

#### What are potential issues with simple word tokenization here?


::: fragment

<br>

### Any ideas how to solve these issues?

:::

## Solution: Subword Tokenization

![Source: [ðŸ¤— LLM Course](https://huggingface.co/learn/llm-course/en/chapter2/4)](vis/subword_tokenization.png)

#### Which advantages do you see with subword tokenization?

## Different Tokenizers

<br>

![Source: [ðŸ¤— LLM Course](https://huggingface.co/learn/llm-course/en/chapter2/4)](vis/different_tokenizers.png)


## How does it work? (roughly)

1. Start with small vocabulary of characters and special tokens
2. Map them in the training data (e.g. "w ##o ##r ##d")
3. Combine most frequent pairs (BPE) or unexpectedly frequent ones, given constitutive terms (WordPiece)
4. Stop when you reach your desired vocabulary size

::: {.callout-important .fragment}
### Note that this works better for some languages than for others
:::

::: aside
More: ðŸ¤— Course [Ch.5](https://huggingface.co/learn/llm-course/en/chapter6/5) and [Ch.6](https://huggingface.co/learn/llm-course/en/chapter6/6)
:::

## Special Tokens

<br>

- `[CLS]`: Represents the entire input sequence
- `[SEP]`: Separates different segments of text
- `[PAD]`: Used for padding sequences to the same length
- `[UNK]`: Represents unknown or out-of-vocabulary words
- `[MASK]`: Used for masked language modeling tasks

# Contextualized Embeddings

The following slides are based @tunstall2022natural, Ch.2.

## Issues with Classic Embeddings

```{r}
#| fig.width: 5
#| fig.height: 3

library(ggplot2)
library(dplyr)

data.frame(
  word = c("flies", "glides", "soars", "insects", "bugs"),
  x = c(0.5, 0.1, 0.2, 0.8, 0.9),
  y = c(0.6, 0.25, 0.3, 0.9, 0.8),
  col = c("blue", "green", "green", "red", "red")
) %>% 
  ggplot(aes(x, y, label = word, color = col)) +
  geom_text() +
  xlim(0, 1) + ylim(0, 1) +
  theme_void() +
  theme(legend.position = "none")

```

## Issues with Classic Embeddings

### Contextual Meaning

<br>

### Sentence a: Time *flies* like an arrow

vs.

### Sentence b: Fruit *flies* like a banana

::: fragment

<br>

### How could we represent the different meanings?

:::

## Contextualized Embeddings

```{r}
#| fig.width: 5
#| fig.height: 3

library(ggplot2)
library(dplyr)

data.frame(
  word = c("flies (a)", "flies (b)", "glides", "soars", "insects", "bugs"),
  x = c(0.3, 0.75, 0.1, 0.2, 0.8, 0.9),
  y = c(0.35, 0.75, 0.25, 0.3, 0.9, 0.8),
  col = c("lightgreen", "lightred", "green", "green", "red", "red")
) %>% 
  ggplot(aes(x, y, label = word, color = col)) +
  geom_text() +
  xlim(0, 1) + ylim(0, 1) +
  theme_void() +
  theme(legend.position = "none")

```

## Contextualized Embeddings

### Contextual Meaning

<br>

### Sentence a: Time *flies* like an arrow

vs.

### Sentence b: Fruit *flies* like a banana

::: fragment

<br>

### How do *you* infer the different meanings?

:::

## Attention Mechanism: Intuition

![Source: @tunstall2022natural, Ch. 2](vis/attention_simple.png)

## Solution: Weighted Average

$$x'_i = \sum_{j=1}^{n} w_{ij} x_j$$

> **Weighted average** of all input embeddings

- $x'_i$: Contextualized embedding of token $i$
- $x_j$: Embedding of token $j$
- $w_{ij}$: Attention weight for token $j$ with respect to token $i$
- $n$: Number of tokens in the input sequence


## Step 1: Create query, key, and value vectors

<br>

- **Query**: Represents the token itself
- **Key**: Represents the context of the token
- **Value**: Again the token itself (more later)

## Step 2: Calculate the attention scores

<br>

> The dot product of the query and key vectors gives us the attention scores/weights

<br>

::: {.callout-important .fragment}

### What does the dot product of two vectors indicate?

:::


## Finalize: Normalize & take average

### Step 3: Normalize the attention scores

> The attention scores are normalized using the "softmax" function to ensure they sum to 1

::: fragment

### Step 4: Multiply the normalized scores with the value vectors and sum them up

$$x'_i = \sum_{j=1}^{n} w_{ij} x_j$$

:::

## Attention Mechanism: Example

![](vis/attention_contextual.png)

## Attention in Practice

<br>

- Query, key, and value vectors are **learned** representations
- Attention calculated for each hidden layer 
- Multiple attention 'heads' are used in parallel
- Outputs combined using another learned linear transformation

## Positional Encodings

<br>

- We can also capture the **position** of each token in the sequence
- Similar approach:
  - Create a vector for each position in the sequence
  - Add these vectors to the token embeddings
  - This allows the model to understand the order of tokens

# Breaktime

# Tutorial I

**Tokenization, Attention & Inference**

[Notebook](https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/04a_tokens_attention.ipynb)

# The Transformer

![](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExcmkzaWZ0eHk1bGs4cHg2ajZ2NG1sMGs1YnIycG5icm4ydmswbG84ZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/4uUnISbiiBAHmwHi4d/giphy.gif)

## Some terminology

::: callout-note

### Encoder (e.g. BERT)

:::: columns
::: column

- Converts an input sequence of tokens into a sequence of embedding vectors
- Trained on masked language modeling
- Tasks: Text classification, named entity recognition, extractive question answering, ...

:::
::: column

![](vis/encoder.png)

:::
::::
:::

::: fragment
::: callout-important

### Decoder (e.g. GPT) - TOMORROW!

:::: columns
::: column

- Uses a sequence of embedding vectors to iteratively generate an output sequence of tokens, one token at a time
- Trained to predict next word
- Tasks: Mainly text **generation** (e.g. Chatbot responses)

:::
::: column

![](vis/decoder.png)

:::
::::
:::
:::

## Transformers Overview

![](vis/transformers.png)

## Encoder Block

![Source: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)](vis/encoder_block.png)

## The Encoder Architecture (e.g. BERT)

![Source: @tunstall2022natural, Ch. 2](vis/encoder_full.png)

## The Encoder Architecture (e.g. BERT)

![Source: @devlin2019bert](vis/bert.png)

## Classification Heads


![Source: [ðŸ¤— LLM Course Ch. 2](https://huggingface.co/learn/llm-course/chapter2/2)](vis/encoder_simple.svg)

## Classification Heads

#### Many Tasks, One Model

- Sequence Classification
- Masked Language Modeling
- Multiple Choice
- Token Classification
- Question Answering

## Additional Considerations

<br>

- **Dropout**: prevent overfitting by randomly setting a fraction of input units to 0 during training
- **Layer normalization**: stabilize and accelerate training by normalizing the inputs to each layer
- **Residual connections**: add input to output (bypass layers)

# Training a Transformer

## What is "Training" ?

<br>

- Remember from ML course: **Training** is the process of optimizing a model's parameters on a specific task using labeled data.
- In regression framework, this is called **fitting** the model to the training data.

## Masked Language Modeling (MLM)

<br>

::: columns
::: column
::: incremental
- Training objective used for models like BERT
- Randomly masks a percentage of input tokens
- Model learns to predict the masked tokens based on context

:::
:::
::: column


![](vis/mlm.png)

:::
:::

. . .

Output: probability distribution across tokens ("Virtual" (87%), "good" (8%), "helpful" (3%))

## Three Essential Steps

<br>

- **Forward pass**: The process of passing input data through the model to obtain predictions.
- **Loss**: A measure of how well the model's predictions match the ground truth.
- **Backward pass**: The process of updating model parameters based on the loss.

## Full Training vs. Fine-Tuning

- Read [this short explainer](https://nicoberk.quarto.pub/training-vs-fine-tuning/) 
- Form groups of 2-3
- Alternate explaining to each other:
  - the difference between **pre-training** and **fine-tuning**
  - how does training a transformer from scratch work?
  - how does fine-tuning work? How do the task and the model differ in this process?
- Collect two benefits and drawbacks of each approach.

## Transfer Learning/Fine-Tuning

![@tunstall2022natural, Fig. 1-7](vis/fine-tuning.png)

## Hyperparameters

<br><br>

> Essentially any aspect of a model that isn't *learned* and needs to be chosen by the researcher.

## Hyperparameters

<br>

#### Many different ones; important for us:

- **Number of Epochs**: The number of complete passes through the training dataset.
- **Batch Size**: The number of training examples utilized in one iteration before updating weights.
- **Learning Rate**: How much should the model update weights in response to loss?

## More Hyperparameters

<br>

- **Weight Decay**: Prevents overfitting by penalizing large weights.
- **Dropout**: Ignores randomly selected neurons during training (prevents overfitting).
- **Seed**: A value used to initialize the random number generator for reproducibility.

. . .

Note: prompts are also hyperparameters!


## Understanding Training Dynamics

<br>

- Read this [guide on understanding learning curves](https://huggingface.co/learn/llm-course/chapter3/5) (ignore the code bits)
- Take the quiz
- Go to this [website visualizing training dynamics](https://playground.tensorflow.org/) and apply your insights
  - Make sure to select the 'whirl' pattern

## Tracking Training Progress

<br>

### Weights and Biases

[Weights and Biases](https://wandb.ai/) - "experiment" tracking and visualization


## The Huggingface Ecosystem

<br>

### [The Model Hub](https://huggingface.co/models)

### [Datasets](https://huggingface.co/datasets)


# Tutorial II

**Fine-tuning a Transformer Model**

[Notebook](https://colab.research.google.com/github/nicolaiberk/llm_ws/blob/main/notebooks/04b_finetuning_bert.ipynb)


## Resources {.uncounted}


