---
title: "Lecture 2: Embeddings"
author: "Nicolai Berk"
subtitle: "CUSO WS on Large Language Models"
date: "2025-09-03"
description: "This lecture covers the concept of embeddings in the context of large language models."
keywords: ["embeddings", "large language models", "machine learning"]
format:
    revealjs:
        theme: simple
        slide-number: true
        toc: false
bibliography: "../references.bib"
---

## Recap: Bag-of Words {auto-animate="true"}

<br>

#### Dance like nobody's watching

| dance | move | steal | like | nobody's | watching |
|-------|------|-------|------|---------|---------|
| 1     | 0    | 0     | 1    | 1       | 1       |


## Recap: Bag-of Words {auto-animate="true" auto-animate-duration="0"}

<br>

#### Move like nobody's watching

| dance | move | steal | like | nobody's | watching |
|-------|------|-------|------|---------|---------|
| 1     | 0    | 0     | 1    | 1       | 1       |
| 0     | 1    | 0     | 1    | 1       | 1       |


## Recap: Bag-of Words {auto-animate="true" auto-animate-duration="0"}

<br>

#### Steal like nobody's watching

| dance | move | steal | like | nobody's | watching |
|-------|------|-------|------|---------|---------|
| 1     | 0    | 0     | 1    | 1       | 1       |
| 0     | 1    | 0     | 1    | 1       | 1       |
| 0     | 0    | 1     | 1    | 1       | 1       |


## Recap: Bag-of Words {auto-animate="true" auto-animate-duration="0"}

<br>

#### Nobody's watching: steal!

| dance | move | steal | like | nobody's | watching |
|-------|------|-------|------|---------|---------|
| 1     | 0    | 0     | 1    | 1       | 1       |
| 0     | 1    | 0     | 1    | 1       | 1       |
| 0     | 0    | 1     | 1    | 1       | 1       |
| 0     | 0    | 1     | 0    | 1       | 1       |


::: fragment

<br>

### What do we miss with this representation?

:::

## Limitations of BoW & Tf-IDF

<br>

::: incremental

- Ignores semantic meaning of words/tokens
- Ignores order of words/tokens
- High dimensionality & sparsity

:::

## Distributional Hypothesis

<br>

#### Challenge: How can we measure *meaning*?

<br>

::: fragment
::: callout-note
### Distributional Hypothesis

**A word's meaning can be inferred from the context it appears in.**


:::

<br>

> @firth1957studies: "know a word by the company it keeps" 

:::



## Example: what is "ziti"?

. . .

![Source: [Your Dictionary](https://sentence.yourdictionary.com/ziti)](vis/ziti_context.png)

::: fragment
::: {.absolute top=100 left=200 width=800}
![Source: Wikipedia](vis/ziti_image.jpg)
:::
:::

## How can we conceptualize this?

### Semantic Space

```{r}

library(ggplot2)
library(dplyr)

data.frame(
        word = c("ziti", "pasta", "lasagna", "macaroni", "car", "bike", "carrot", "apple", "banana", "orange"),
        dim1 = c(0.8, 0.9, 0.75, 0.7, 0.1, 0.2, 0.35, 0.3, 0.4, 0.35),
        dim2 = c(0.1, 0.2, 0.15, 0.05, 0.9, 0.8, 0.15, 0.2, 0.1, 0.25),
        highlight = c(TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE),
        stringsAsFactors = FALSE
    ) %>% 
    ggplot(aes(x = dim1, y = dim2, label = word, color = !highlight)) +
    geom_text(size = 5) +
    theme_void() +
    theme(legend.position = "none")



```


## How do we put this into numbers?

### Two (and a half) approaches:

::: incremental

1. Train a **model** to predict (word2vec)
   i. a word given its context (skip-gram)
   ii. the context given a word (CBOW)
2. **Co-occurrence matrix** (GloVe)
    - assess how often do words appear together
    - minimize the difference in representations of words that appear in similar contexts

:::

::: fragment

### Identical result: word embeddings

:::

# `word2vec` (skip-gram)

@mikolov2013efficient

## 1. Identify word context^[The following slides are based on [Chris McCormick's excellent tutorial](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).]

![](http://mccormickml.com/assets/word2vec/training_data.png){height=450}

::: {.absolute top=75 right=220}
::: {data-id="box1" style="background: #ffffffff; width: 320px; height: 460px; margin: 10px;"}
:::
:::

## 2. Build training data

![](http://mccormickml.com/assets/word2vec/training_data.png){height=450}


## 3. Train a model

![](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)

## Result

::: {.absolute top=75 left=150}
![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)
:::

::: {.fragment .absolute .fade-out top=75 right=150}
::: {data-id="box1" style="background: #ffffffff; width: 420px; height: 800px; margin: 10px;"}
:::
:::

::: {.fragment .absolute bottom=0}
### <mark>Weight matrix = word embeddings!</mark>
:::

## Training Data and Bias

- Embeddings simply reflect the data they are trained on
- If biases exist in the data, they will be reflected in the embeddings
- This is a problem, e.g. when using embeddings for downstream tasks
- However, this also enables us to study these biases [@kozlowski2019geometry; @kroon2021guilty]

#### Thinking about the data we use is essential!

# Working with Embeddings

## Similar context $\rightarrow$ similar embeddings

![](vis/example_embs.png)

## Vector Basics

![Source: [Math Insight](https://mathinsight.org/vector_introduction)](vis/dir_mag.png)

## Vector Basics

<br>

Magnitude: length of a vector

<br>

$$||\mathbf{a}|| = \sqrt{a_1^2 + a_2^2 + \ldots + a_n^2}$$


## Vector Addition

<br>

$$\mathbf{a} + \mathbf{b} = a_1 + b_1, a_2 + b_2 + \ldots, a_n + b_n$$

<br>

::: fragment

$$
\mathbf{a} + \mathbf{b} = [1,3] + [3,2] = [4,5]
$$

:::

## Vector Addition

### Visually

```{r}
#| fig.width: 4
#| fig.height: 4
#| fig.align: "center"

data.frame(
  vector = c("a", "b"),
  x = c(0, 0),
  y = c(0, 0),
  xend = c(1, 3),
  yend = c(3, 2)
) %>%
  ggplot(aes(xend = xend, yend = yend, col = vector)) +
  geom_segment(aes(x = x, y = y, linetype = vector == "a + b"), arrow = arrow(length = unit(0.2, "inches"))) +
  geom_text(aes(x = xend, y = yend, label = vector), vjust = -1) +
  theme_minimal() +
  theme(legend.position = "none") +
  ylim(-1, 5.5) +
  xlim(-1, 5)

```

## Vector Addition

### Visually

```{r}
#| fig.width: 4
#| fig.height: 4
#| fig.align: "center"

data.frame(
  vector = c("a", "b", "a + b"),
  x = c(0, 0, 1),
  y = c(0, 0, 3),
  xend = c(1, 3, 4),
  yend = c(3, 2, 5)
) %>%
  ggplot(aes(xend = xend, yend = yend, col = vector)) +
  geom_segment(aes(x = x, y = y, linetype = vector == "a + b"), arrow = arrow(length = unit(0.2, "inches"))) +
  geom_segment(
    aes(x = 1.75, y = 1.5, xend = 2, yend = 3.5), 
    arrow = arrow(length = unit(0.2, "inches"))) +
  geom_text(aes(x = xend, y = yend, label = vector), vjust = -1) +
  geom_text(aes(x = 2, y = 2.5, label = "add b to a"), hjust = -1) +
  theme_minimal() +
  theme(legend.position = "none") +
  ylim(-1, 5.5) +
  xlim(-1, 5)


```

## Vector Addition

### Visually

```{r}
#| fig.width: 4
#| fig.height: 4
#| fig.align: "center"

data.frame(
  vector = c("a", "b", "a + b"),
  x = c(0, 0, 0),
  y = c(0, 0, 0),
  xend = c(1, 3, 4),
  yend = c(3, 2, 5)
) %>%
  ggplot(aes(xend = xend, yend = yend, col = vector)) +
  geom_segment(aes(x = x, y = y, linetype = vector == "a + b"), arrow = arrow(length = unit(0.2, "inches"))) +
  geom_text(aes(x = xend, y = yend, label = vector), vjust = -1) +
  theme_minimal() +
  theme(legend.position = "none") +
  ylim(-1, 5.5) +
  xlim(-1, 5)


```


## Basic Operations: Examples

<br>

#### Paris is to France as X is to Germany

$WV_{Paris} - WV_{France} + WV_{Germany} = WV_{X}$

::: fragment

![](vis/example_embs_2.png)

:::

# 5 minute break

Then we get our hands dirty!

# Embeddings: Tutorial 1

[Notebook]()

## Advanced Operations: Projection

### Example from @kozlowski2019geometry

![](vis/projection_example.png)

## Advanced Operations: Projection

### Visually

![Neat interactive visualization: [Math Insight](https://mathinsight.org/dot_product)](vis/projection.png)

## Advanced Operations: Projection

### Dot Product

$$\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \ldots + a_n b_n$$

::: fragment

$$\mathbf{a} \cdot \mathbf{b} = 1 * 3 + 3 * 2 = 3 + 6 = 9$$

:::


## Advanced Operations: Projection

### Normalize by magnitude of $\mathbf{b}$

<br>

$$\frac{9}{||\mathbf{b}||} = \frac{9}{\sqrt{3^2 + 2^2}} = \frac{9}{\sqrt{13}} \approx 2.5$$

## Advanced Operations: Projection

### Visually

```{r}
#| fig.width: 4
#| fig.height: 4
#| fig.align: "center"

a = c(1, 3)
b = c(3, 2)

projection = sum(a * b) / sqrt(sum(b^2))
proj_vec = (projection / sqrt(sum(b^2))) * b

data.frame(
  vector = c("a", "b", "projection"),
  x = c(0, 0, 0),
  y = c(0, 0, 0),
  xend = c(a[1], b[1], proj_vec[1]),
  yend = c(a[2], b[2], proj_vec[2])
) %>%
  ggplot(aes(col = vector)) +
  geom_segment(aes(xend = xend, yend = yend, x = x, y = y), arrow = arrow(length = unit(0.2, "inches"))) +
  geom_text(aes(x = xend, y = yend, label = vector), vjust = -1) +
  theme_minimal() +
  theme(legend.position = "none") +
  ylim(-1, 5.5) +
  xlim(-1, 5)

```


## Cosine Similarity

$$\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| |\mathbf{b}|} $$

::: fragment

Seems familiar? Remember Projection:

$$\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{b}|} $$

Difference: normalization by magnitude of *both* vectors.

:::

::: fragment

####  Hence bounded in interval $[-1, 1]$

:::

## Cosine Similarity

$$\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| |\mathbf{b}|} $$

#### What is $\theta$?

Neat feature: $\theta$ is the angle between our two vectors!

## Cosine Similarity: Intuition

```{r}
#| fig.width: 6
#| fig.height: 4
#| fig.align: centre

plot_cossim <- function(a,b){
    cos_sim <- sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))
    df <- t(data.frame(a, b))
    colnames(df) <- c('x', 'y')
    df <- as.data.frame(df)
    df$vec <- c('a', 'b')

    ggplot(df, aes(col = vec)) +
    geom_segment(aes(xend = x, yend = y, x = 0, y = 0), arrow = arrow(length = unit(0.2, "inches"))) +
    geom_text(aes(x = x, y = y, label = vec), vjust = -1) +
    theme_minimal() +
    theme(legend.position = "none") +
    ggtitle(paste("Cosine Similarity:", round(cos_sim, 2))) +
    ylim(-1, 5) +
    xlim(-5, 5)
}

a <- c(4,0)
b <- c(4,1)

plot_cossim(a,b)

```

## Cosine Similarity: Intuition

```{r}
#| fig.width: 6
#| fig.height: 4
#| fig.align: centre

a <- c(4,0)
b <- c(0,1)

plot_cossim(a,b)
```

## Cosine Similarity: Intuition

```{r}
#| fig.width: 6
#| fig.height: 4
#| fig.align: centre

a <- c(4,0)
b <- c(-4,1)

plot_cossim(a,b)
```


## Defining Semantic Axes

Nice feature of all this algebra: you can define semantic *axes* simply by substracting vectors of polar opposites!

$$WV_{frenchness} = WV_{french} - WV_{german}$$

::: fragment

:::: columns
::: column

The **projection** of a term on this vector tells us which pole it is more associated with!

:::
::: column

| Term | Frenchness |
|------|-------------|
| Paris | 0.298 |
| Lausanne | 0.073 |
| Bern | -0.177 |
| Berlin | -0.333 |

:::
::::
:::

# Second Tutorial here

## Tracking Changes in Meaning over Time

#### Chronologically trained embeddings [@rodman2020timely]

- Train new model on each time slice
- Initialize training for time slice $t$ with embeddings from time slice $t-1$

::: fragment

#### Embedding regression [@rodriguez2023embedding]

- Uses a multivariate regression framework to model embeddings.
- Makes efficient use of scarce data.
- Allows for hypothesis testing $\rightarrow$ many use cases!

:::

## Document Embeddings

> "The paragraph token can be thought of as another word." (p.3)

![Figure 2 from @le2014distributed](vis/paragraph_vector.png)

## Other Embeddings

<br>

#### Any collection of texts can be a document!

- Paragraphs
- Documents
- Speakers [@rheault2020word]
- Countries
- ...

## State of the Art Embeddings

<br>

#### Sentence Transformers

- Based on the Transformer architecture.
- Pre-trained on large corpora.
- Fine-tuned for specific tasks.

### Tomorrow!


## Note of Caution: Validation

- Many researcher degrees of freedom: seed words, hyperparameters, etc.
- Risk of accommodating own biases and cherry picking 

$\rightarrow$ **Validation is essential!**

- Precise method will depend on the task at hand.
- Correlate with established measures
- Gold standard remains human assessment

### Also more tomorrow!

## Further reading

# See you tomorrow!

![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDloNmo0MzVjdTZheXczcm43a3M0ZW1raTk5M2tvN253eWpzb2NxbCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/3o6Mb3PGeR7EufOgYo/giphy.gif)

## References {.unnumbered}


