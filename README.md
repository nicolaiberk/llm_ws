# CUSO Workshop: LLMs for Social Scientists

3-5 September 2025, UNIL Lausanne

This workshop provides an introduction to fundamentals of NLP and LLMs for social scientists. It covers basic and advanced text representation, fundamentals of machine learning, transformer architectures, and applied questions regarding LLMs. The course consists of six three-hour sessions, each divided into a lecture with a conceptual focus, and a tutorial covering the implementation in python. The course is designed to provide a fast overview of major topics in the application of LLMs. It covers most content rather superficially, aiming to provide students with a good intuition of each concept and code as a starting point to implement their own ideas.

## Wednesday, Sept. 3rd

### Morning Session (10:30 - 13:30)

Intro to Python & Text Representation

### Afternoon Session (14:30 - 17:30)

Intro to Embeddings

## Thursday, Sept. 4th

### Morning Session (9:30 - 12:30)

Intro to Machine Learning

### Afternoon Session (9:30 - 12:30)

Intro to Transformer Models


## Friday, Sept. 5th

### Morning Session (9:30 - 12:30)

Generative Transformers

### Afternoon Session (13:30 - 16:30)

Using LLMs in Social Science Research/tbd

<!-- 

## Day 1

### Introduction & Representing Text

*Lecture I: Introduction & Applications of NLP in the Social Sciences

*Tutorial I: Intro to Python

*Lecture II: Representing Text: Bag-of-Words

*Tutorial II: Bag-of-Words and Scaling

### Embeddings

#### Lecture I: Working with Embeddings

[Slides]()

#### Tutorial I: Intro to Word Embeddings with `gensim`

[Notebook]()

Lecture II: Advanced Embeddings

Tutorial II: Scaling Word Embeddings & Document Embeddings

## Day 2

### Supervised Machine Learning

Lecture I: The basic process of Supervised Machine Learning & Bias-variance tradeoffs

Tutorial I: Supervised ML with `scikit-learn`

Lecture II: Basics of Neural Networks

*Tutorial II: Classification with embeddings, hackathon: best model with and without embeddings

### Introduction to Transformers: The Encoder

Lecture I: Advanced Tokenization & Contextualized Embeddings

Tutorial I: Tokenization, attention, inference with transformers

Lecture II: The Encoder, Training a Transformer

Tutorial II: Fine-tune your own BERT model

## Day 3

### Advanced NLP & the Decoder

*Lecture I: ?? (Decoder architecture, training generative models, hyperparameters)

*Tutorial I: ?? (tracking your experiments/hyperparameter tuning with wandb)

*Lecture II: ?? (Climate Impact of LLMs & PEFT, Bias & Debiasing, ...)

*Tutorial II: ?? (PEFT?)

### RAG & other Shananigans, Q&A

*Lecture I: ?? ()

*Tutorial I: ?? (tracking your experiments/hyperparameter tuning with wandb?)

*Lecture II: ?? ()

*Tutorial II: ?? (Building your own chatbot with RAG?) -->
